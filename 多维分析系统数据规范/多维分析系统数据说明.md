# 车险变动成本多维分析系统 - 数据综合说明文档

## 📋 文档概要

### 主要组成部分
1. **数据架构概述** - 完整的数据结构和文件组织说明
2. **字段分类体系** - 17个筛选维度 + 9个绝对值字段 + 7个计算字段的详细定义
3. **指标计算方法** - 各类指标的计算公式和代码实现示例
4. **AI开发指南** - 针对AI系统的数据加载、处理和分析最佳实践
5. **应用场景示例** - 典型的业务分析场景和代码实现

### 关键信息点
- **数据规模**: 每周约16,000行 × 23个存储字段，当前覆盖最近2年数据
- **数据周期**: 考虑保单1年期限和全年分布生效特点，完整追踪需覆盖105周
- **核心特性**: 绝对值存储 + 实时计算，确保聚合分析的准确性
- **技术栈**: Python + Pandas + 专用计算工具
- **应用目标**: 支撑车险变动成本的多维度分析和可视化

---

## 1. 数据架构概述

### 1.1 数据存储架构

#### 1.1.1 动态目录结构

```
data4dash/                           # 专门为仪表板提供的数据
├── data-{YYYY}/                    # 年度数据文件夹（动态创建）
│   └── {YYYY}保单第{WW}周变动成本明细表.csv  # 周次数据文件（按需生成）
├── metadata/                       # 元数据管理
│   ├── available_years.json        # 可用年度列表
│   ├── available_weeks.json        # 各年度可用周次
│   └── data_catalog.json          # 数据目录索引
└── archive/                        # 历史数据归档（可选）
    └── data-{YYYY}/               # 归档的历史年度数据
```

#### 1.1.2 文件命名规范

**标准格式**: `{年度}保单第{周次}周变动成本明细表.csv`
- **年度**: 4位数字，如2024、2025、2026等
- **周次**: 2位数字，范围1-53，如28、29、30等
- **示例**: `2024保单第28周变动成本明细表.csv`

#### 1.1.3 当前数据状态（示例）

```
data4dash/
├── data-2024/                      # 2024年度数据
│   ├── 2024保单第28周变动成本明细表.csv
│   ├── 2024保单第29周变动成本明细表.csv
│   ├── 2024保单第30周变动成本明细表.csv
│   ├── 2024保单第31周变动成本明细表.csv
│   ├── 2024保单第33周变动成本明细表.csv  # 注：第32周数据缺失
│   ├── 2024保单第34周变动成本明细表.csv
│   ├── 2024保单第35周变动成本明细表.csv
│   └── 2024保单第36周变动成本明细表.csv
└── data-2025/                      # 2025年度数据
    ├── 2025保单第28周变动成本明细表.csv
    ├── 2025保单第29周变动成本明细表.csv
    ├── 2025保单第30周变动成本明细表.csv
    ├── 2025保单第31周变动成本明细表.csv
    ├── 2025保单第33周变动成本明细表.csv
    ├── 2025保单第34周变动成本明细表.csv
    ├── 2025保单第35周变动成本明细表.csv
    └── 2025保单第36周变动成本明细表.csv
```

#### 1.1.4 自动发现机制

```python
import os
import glob
import json
from pathlib import Path

def discover_data_structure(base_path='data4dash'):
    """
    自动发现数据结构，适应动态变化
    """
    structure = {
        'available_years': [],
        'year_week_mapping': {},
        'total_files': 0,
        'missing_weeks': {},
        'last_updated': None
    }
    
    # 扫描年度文件夹
    year_dirs = glob.glob(f'{base_path}/data-*')
    
    for year_dir in sorted(year_dirs):
        year = int(os.path.basename(year_dir).split('-')[1])
        structure['available_years'].append(year)
        
        # 扫描周次文件
        week_files = glob.glob(f'{year_dir}/*.csv')
        weeks = []
        
        for file_path in week_files:
            filename = os.path.basename(file_path)
            # 提取周次信息
            week_match = re.search(r'第(\d+)周', filename)
            if week_match:
                week = int(week_match.group(1))
                weeks.append(week)
        
        structure['year_week_mapping'][year] = sorted(weeks)
        structure['total_files'] += len(weeks)
        
        # 检测缺失周次
        if weeks:
            min_week, max_week = min(weeks), max(weeks)
            expected_weeks = set(range(min_week, max_week + 1))
            actual_weeks = set(weeks)
            missing = expected_weeks - actual_weeks
            if missing:
                structure['missing_weeks'][year] = sorted(list(missing))
    
    return structure

# 使用示例
data_structure = discover_data_structure()
print(f"发现 {len(data_structure['available_years'])} 个年度的数据")
print(f"总计 {data_structure['total_files']} 个数据文件")
```

### 1.2 数据规模特征

| 维度 | 数值 | 说明 |
|------|------|------|
| **当前文件数** | 16个 | 最近2年 × 8周 |
| **单周数据量** | ~16,000行 | 每文件约16,000行 |
| **存储字段数** | 23个 | 17维度 + 6绝对值 |
| **计算字段数** | 7个 | 实时计算获得 |
| **理论数据周期** | 105周 | 完整追踪1年期保单所需 |
| **当前时间跨度** | 最近2年 | 第28-36周（缺32周） |
| **地域属性** | 成都、中支 | 三级机构是否在省会 |
| **险种类型** | 商业险、交强险 | 商业保险、强制保险 |

#### 数据周期说明
**保单期限特性**: 车险保单期限为1年，但保单生效日期分布在全年任何时间
- 1月1日生效保单 → 当年12月31日到期
- 12月31日生效保单 → 次年12月30日到期
- **完整追踪**: 某年度保单数据需覆盖105周（约2年+1周）
- **当前覆盖**: 基于业务需要选择关键周次进行分析

### 1.3 数据重构特点 <mcreference link="https://docs.kapa.ai/improving/writing-best-practices" index="1">1</mcreference>

- **字段分离**: 筛选维度与绝对值字段分离，避免率值直接聚合
- **单位统一**: 所有金额字段统一为"元"单位
- **结构优化**: 移除冗余字段，保留核心业务数据
- **计算安全**: 通过实时计算确保聚合场景下的准确性
- **扩展设计**: 支持历史数据年份扩展，适应长期数据积累需求

---

## 2. 字段分类体系

### 2.1 筛选维度字段（17个）

这些字段用于数据筛选、分组和多维分析：

#### 2.1.1 时间维度
| 字段名 | 数据类型 | 选项数 | 示例值 | 描述 |
|--------|----------|--------|--------|---------|
| `snapshot_date` | 日期 | - | 2025-07-13 | 数据快照日期 |
| `policy_start_year` | 整数 | 2 | 2024, 2025 | 保单起期年度 |
| `week_number` | 整数 | 8 | 28, 29, 30, 31, 33, 34, 35, 36 | 周序号 |

#### 2.1.2 机构维度
| 字段名 | 数据类型 | 选项数 | 主要选项 | 描述 |
|--------|----------|--------|----------|---------|
| `chengdu_branch` | 字符串 | 2 | 成都, 中支 | 机构层级 |
| `third_level_organization` | 字符串 | 13 | 乐山, 天府, 宜宾, 德阳, 新都, 本部, 武侯, 泸州, 自贡, 资阳, 达州, 青羊, 高新 | 三级机构 |

#### 2.1.3 业务维度
| 字段名 | 数据类型 | 选项数 | 主要选项 | 描述 |
|--------|----------|--------|----------|---------|
| `business_type_category` | 字符串 | 16 | 10吨以上-普货, 非营业客车新车, 营业货车等 | 业务类型分类 |
| `customer_category_3` | 字符串 | 11 | 营业货车, 非营业个人客车, 非营业企业客车等 | 客户三级分类 |
| `insurance_type` | 字符串 | 2 | 商业保险, 交强险 | 险种类型 |
| `coverage_type` | 字符串 | 3 | 主全, 交三, 单交 | 险别组合 |
| `renewal_status` | 字符串 | 3 | 新保, 续保, 转保 | 新续转状态 |
| `terminal_source` | 字符串 | 7 | 0101柜面, 0105微信, 0106移动展业等 | 投保终端来源 |

#### 2.1.4 车辆属性维度
| 字段名 | 数据类型 | 选项数 | 主要选项 | 描述 |
|--------|----------|--------|----------|---------|
| `is_new_energy_vehicle` | 布尔 | 2 | True, False | 是否新能源车 |
| `is_transferred_vehicle` | 布尔 | 2 | True, False | 是否过户车 |

#### 2.1.5 风险评级维度
| 字段名 | 数据类型 | 选项数 | 主要选项 | 描述 |
|--------|----------|--------|----------|---------|
| `vehicle_insurance_grade` | 字符串 | 8 | A, B, C, D, E, F, G, X | 车险分等级 |
| `highway_risk_grade` | 字符串 | 6 | A, B, C, D, E, X | 高速风险等级 |
| `large_truck_score` | 字符串 | 6 | A, B, C, D, E, X | 大货车评分 |
| `small_truck_score` | 字符串 | 6 | A, B, C, D, E, X | 小货车评分 |

### 2.2 绝对值字段（9个）

这些字段存储绝对值数据，用于聚合计算：

| 字段名 | 数据类型 | 单位 | 示例值 | 描述 |
|--------|----------|------|--------|---------|
| `signed_premium_yuan` | 浮点数 | 元 | 6762.26 | 签单保费 |
| `matured_premium_yuan` | 浮点数 | 元 | 2617.05 | 满期保费 |
| `commercial_premium_before_discount_yuan` | 浮点数 | 元 | 0.0 | 商业险折前保费 |
| `policy_count` | 整数 | 件 | 1 | 保单件数 |
| `claim_case_count` | 整数 | 件 | 0 | 赔案件数 |
| `reported_claim_payment_yuan` | 浮点数 | 元 | 0.0 | 已报告赔款 |
| `expense_amount_yuan` | 浮点数 | 元 | 202.87 | 费用金额 |
| `premium_plan_yuan` | 浮点数 | 元 | 8000.0 | 保费计划 |
| `marginal_contribution_amount_yuan` | 浮点数 | 元 | 1500.0 | 满期边际贡献额（非终极） |

### 2.3 计算字段（7个）

这些字段通过实时计算获得，不存储在文件中：

| 字段名 | 计算公式 | 单位 | 描述 |
|--------|----------|------|---------|
| `average_premium_per_policy_yuan` | signed_premium_yuan ÷ policy_count | 元/件 | 单均保费 |
| `claim_frequency_percent` | (claim_case_count ÷ policy_count) × 100 | % | 满期出险率 |
| `average_claim_payment_yuan` | reported_claim_payment_yuan ÷ claim_case_count | 元/件 | 案均赔款 |
| expired_loss_ratio_percent | (reported_claim_payment_yuan ÷ matured_premium_yuan) × 100 | % | 满期赔付率 |
| `expense_ratio_percent` | (expense_amount_yuan ÷ signed_premium_yuan) × 100 | % | 费用率 |
| `variable_cost_ratio_percent` | ((expense_amount_yuan + reported_claim_payment_yuan) ÷ signed_premium_yuan) × 100 | % | 变动成本率 |
| `commercial_auto_underwriting_factor` | signed_premium_yuan ÷ commercial_premium_before_discount_yuan | 系数 | 商业险自主定价系数 |

---

## 3. 指标计算方法

### 3.1 基础计算原则

为确保聚合分析的准确性，所有指标计算都遵循以下原则：

```python
import pandas as pd
import numpy as np

def calculate_insurance_metrics(df):
    """
    计算车险核心指标
    
    参数:
    df: DataFrame - 包含基础数据的DataFrame
    
    返回:
    DataFrame - 包含所有计算指标的DataFrame
    """
    
    # 创建结果DataFrame副本
    result_df = df.copy()
    
    # 1. 基础均值指标
    result_df['average_premium_per_policy_yuan'] = (
        result_df['signed_premium_yuan'] / result_df['policy_count']
    )
    
    # 处理除零情况
    result_df['average_claim_payment_yuan'] = np.where(
        result_df['claim_case_count'] > 0,
        result_df['reported_claim_payment_yuan'] / result_df['claim_case_count'],
        0
    )
    
    # 2. 比率指标（百分比）
    result_df['expense_ratio_percent'] = (
        result_df['expense_amount_yuan'] / result_df['signed_premium_yuan'] * 100
    )
    
    result_df['maturity_ratio_percent'] = (
        result_df['matured_premium_yuan'] / result_df['signed_premium_yuan'] * 100
    )
    
    result_df['claim_frequency_percent'] = (
        result_df['claim_case_count'] / result_df['policy_count'] * 
        result_df['matured_premium_yuan'] / result_df['signed_premium_yuan'] * 100
    )
    
    result_df['matured_loss_ratio_percent'] = np.where(
        result_df['matured_premium_yuan'] > 0,
        result_df['reported_claim_payment_yuan'] / result_df['matured_premium_yuan'] * 100,
        0
    )
    
    result_df['variable_cost_ratio_percent'] = (
        result_df['expense_amount_yuan'] / result_df['signed_premium_yuan'] + 
        result_df['reported_claim_payment_yuan'] / result_df['matured_premium_yuan']
    ) * 100
    
    result_df['marginal_contribution_ratio_percent'] = (
        result_df['marginal_contribution_amount_yuan'] / result_df['matured_premium_yuan'] * 100
    )
    
    result_df['premium_time_progress_achievement_rate_percent'] = (
        result_df['signed_premium_yuan'] / result_df['premium_plan_yuan'] * 100
    )
    
    # 3. 系数指标
    result_df['commercial_auto_underwriting_factor'] = np.where(
        (result_df['insurance_type'] == '商业保险') & 
        (result_df['commercial_premium_before_discount_yuan'] > 0),
        result_df['signed_premium_yuan'] / result_df['commercial_premium_before_discount_yuan'],
        np.nan
    )
    
    return result_df
```

### 3.2 核心计算公式详解

#### 3.2.1 基础指标公式
```python
# 单均保费
average_premium_per_policy = signed_premium_yuan / policy_count

# 满期出险率（包含满期率修正）
claim_frequency_percent = (claim_case_count / policy_count) * (matured_premium_yuan / signed_premium_yuan) * 100

# 案均赔款
average_claim_payment = reported_claim_payment_yuan / claim_case_count

# 满期赔付率
matured_loss_ratio_percent = (reported_claim_payment_yuan / matured_premium_yuan) * 100
```

#### 3.2.2 复合指标公式
```python
# 费用率
expense_ratio_percent = (expense_amount_yuan / signed_premium_yuan) * 100

# 变动成本率
variable_cost_ratio_percent = (
    (expense_amount_yuan / signed_premium_yuan) + 
    (reported_claim_payment_yuan / matured_premium_yuan)
) * 100

# 边际贡献率
marginal_contribution_ratio_percent = (marginal_contribution_amount_yuan / matured_premium_yuan) * 100

# 商业险自主定价系数（仅适用于商业险）
commercial_auto_underwriting_factor = signed_premium_yuan / commercial_premium_before_discount_yuan
```

#### 3.2.3 完整计算示例
```python
def calculate_single_record_metrics(row):
    """
    计算单条记录的所有指标
    """
    metrics = {}
    
    # 基础指标
    metrics['average_premium_per_policy_yuan'] = row['signed_premium_yuan'] / row['policy_count']
    
    if row['claim_case_count'] > 0:
        metrics['average_claim_payment_yuan'] = row['reported_claim_payment_yuan'] / row['claim_case_count']
    else:
        metrics['average_claim_payment_yuan'] = 0
    
    # 比率指标
    metrics['expense_ratio_percent'] = (row['expense_amount_yuan'] / row['signed_premium_yuan']) * 100
    metrics['maturity_ratio_percent'] = (row['matured_premium_yuan'] / row['signed_premium_yuan']) * 100
    
    # 满期出险率（包含满期率修正）
    metrics['claim_frequency_percent'] = (
        (row['claim_case_count'] / row['policy_count']) * 
        (row['matured_premium_yuan'] / row['signed_premium_yuan']) * 100
    )
    
    if row['matured_premium_yuan'] > 0:
        metrics['matured_loss_ratio_percent'] = (row['reported_claim_payment_yuan'] / row['matured_premium_yuan']) * 100
    else:
        metrics['matured_loss_ratio_percent'] = 0
    
    # 变动成本率
    metrics['variable_cost_ratio_percent'] = (
        (row['expense_amount_yuan'] / row['signed_premium_yuan']) + 
        (row['reported_claim_payment_yuan'] / row['matured_premium_yuan'])
    ) * 100
    
    # 边际贡献率
    if row['matured_premium_yuan'] > 0:
        metrics['marginal_contribution_ratio_percent'] = (row['marginal_contribution_amount_yuan'] / row['matured_premium_yuan']) * 100
    else:
        metrics['marginal_contribution_ratio_percent'] = 0
    
    # 保费时间进度达成率
    if row['premium_plan_yuan'] > 0:
        metrics['premium_time_progress_achievement_rate_percent'] = (row['signed_premium_yuan'] / row['premium_plan_yuan']) * 100
    else:
        metrics['premium_time_progress_achievement_rate_percent'] = 0
    
    # 商业险自主定价系数
    if row['insurance_type'] == '商业保险' and row['commercial_premium_before_discount_yuan'] > 0:
        metrics['commercial_auto_underwriting_factor'] = row['signed_premium_yuan'] / row['commercial_premium_before_discount_yuan']
    else:
        metrics['commercial_auto_underwriting_factor'] = None
    
    return metrics
```

### 3.3 聚合计算最佳实践

**重要原则**: 先聚合绝对值，再计算比率

```python
def aggregate_and_calculate(df, group_by_fields):
    """
    正确的聚合计算方式
    
    参数:
    df: DataFrame - 原始数据
    group_by_fields: list - 分组字段列表
    
    返回:
    DataFrame - 聚合后的结果，包含重新计算的指标
    """
    
    # 1. 先聚合绝对值字段
    agg_dict = {
        'signed_premium_yuan': 'sum',
        'matured_premium_yuan': 'sum', 
        'commercial_premium_before_discount_yuan': 'sum',
        'policy_count': 'sum',
        'claim_case_count': 'sum',
        'reported_claim_payment_yuan': 'sum',
        'expense_amount_yuan': 'sum',
        'premium_plan_yuan': 'sum',
        'marginal_contribution_amount_yuan': 'sum'
    }
    
    agg_result = df.groupby(group_by_fields).agg(agg_dict).reset_index()
    
    # 2. 基于聚合后的绝对值重新计算所有指标
    agg_result = calculate_insurance_metrics(agg_result)
    
    return agg_result

# ✅ 正确的使用方式
grouped_data = aggregate_and_calculate(df, ['chengdu_branch', 'insurance_type'])

# ❌ 错误的方式（直接聚合比率会导致计算错误）
# wrong_result = df.groupby(['chengdu_branch', 'insurance_type'])['matured_loss_ratio_percent'].mean()
```

### 3.2 核心计算公式

#### 3.2.1 基础指标公式
```python
# 单均保费
average_premium_per_policy = signed_premium_yuan / policy_count

# 满期出险率
claim_frequency_percent = (claim_case_count / policy_count) * 100

# 案均赔款
average_claim_payment = reported_claim_payment_yuan / claim_case_count

# 满期赔付率
expired_loss_ratio_percent = (reported_claim_payment_yuan / matured_premium_yuan) * 100
```

#### 3.2.2 复合指标公式
```python
# 费用率（需要费用金额字段）
expense_ratio_percent = (expense_amount_yuan / signed_premium_yuan) * 100

# 变动成本率
variable_cost_ratio_percent = ((expense_amount_yuan + reported_claim_payment_yuan) / signed_premium_yuan) * 100

# 商业险自主定价系数
commercial_auto_underwriting_factor = signed_premium_yuan / commercial_premium_before_discount_yuan
```

### 3.3 聚合计算最佳实践 <mcreference link="https://www.askwisdom.ai/ai-data-preparation" index="3">3</mcreference>

**重要原则**: 先聚合绝对值，再计算比率

```python
def aggregate_and_calculate_metrics(df, group_by_fields):
    """
    正确的聚合计算方法
    """
    # 1. 先聚合绝对值字段
    agg_result = df.groupby(group_by_fields).agg({
        'signed_premium_yuan': 'sum',
        'matured_premium_yuan': 'sum',
        'reported_claim_payment_yuan': 'sum',
        'expense_amount_yuan': 'sum',
        'marginal_contribution_amount_yuan': 'sum',
        'premium_plan_yuan': 'sum',
        'commercial_premium_before_discount_yuan': 'sum',
        'policy_count': 'sum',
        'claim_case_count': 'sum'
    }).reset_index()
    
    # 2. 基于聚合后的绝对值重新计算比率
    agg_result['expired_loss_ratio_percent'] = (
        agg_result['reported_claim_payment_yuan'] / 
        agg_result['matured_premium_yuan'] * 100
    )
    
    agg_result['expense_ratio_percent'] = (
        agg_result['expense_amount_yuan'] / 
        agg_result['signed_premium_yuan'] * 100
    )
    
    agg_result['claim_frequency_percent'] = (
        (agg_result['claim_case_count'] / agg_result['policy_count']) * 
        (agg_result['matured_premium_yuan'] / agg_result['signed_premium_yuan']) * 100
    )
    
    agg_result['average_premium_per_policy_yuan'] = (
        agg_result['signed_premium_yuan'] / agg_result['policy_count']
    )
    
    # 处理除零情况
    agg_result['average_claim_payment_yuan'] = np.where(
        agg_result['claim_case_count'] > 0,
        agg_result['reported_claim_payment_yuan'] / agg_result['claim_case_count'],
        0
    )
    
    return agg_result

# ❌ 错误的聚合方式（直接聚合比率）
def wrong_aggregation(df, group_by_fields):
    """
    这种方式会导致计算错误，不要使用
    """
    return df.groupby(group_by_fields)['expired_loss_ratio_percent'].mean()
```

---

## 4. AI开发指南

### 4.1 数据加载策略 <mcreference link="https://docs.kapa.ai/improving/writing-best-practices" index="1">1</mcreference>

#### 4.1.1 智能数据发现与加载

```python
import pandas as pd
import glob
import re
from pathlib import Path
from datetime import datetime

class DataLoader:
    """
    智能数据加载器，自动适应数据结构变化
    """
    
    def __init__(self, base_path='data4dash'):
        self.base_path = Path(base_path)
        self.data_structure = self.discover_structure()
    
    def discover_structure(self):
        """自动发现数据结构"""
        structure = {
            'available_years': [],
            'year_week_mapping': {},
            'file_paths': {},
            'last_scan': datetime.now().isoformat()
        }
        
        # 扫描所有年度文件夹
        for year_dir in sorted(self.base_path.glob('data-*')):
            year_match = re.search(r'data-(\d{4})', year_dir.name)
            if not year_match:
                continue
                
            year = int(year_match.group(1))
            structure['available_years'].append(year)
            structure['year_week_mapping'][year] = []
            structure['file_paths'][year] = {}
            
            # 扫描周次文件
            for csv_file in sorted(year_dir.glob('*.csv')):
                week_match = re.search(r'第(\d+)周', csv_file.name)
                if week_match:
                    week = int(week_match.group(1))
                    structure['year_week_mapping'][year].append(week)
                    structure['file_paths'][year][week] = str(csv_file)
        
        return structure
    
    def load_single_file(self, year, week):
        """加载单个文件"""
        if year not in self.data_structure['file_paths']:
            raise ValueError(f"年度 {year} 的数据不存在")
        
        if week not in self.data_structure['file_paths'][year]:
            available_weeks = self.data_structure['year_week_mapping'][year]
            raise ValueError(f"第 {week} 周数据不存在，可用周次: {available_weeks}")
        
        file_path = self.data_structure['file_paths'][year][week]
        df = pd.read_csv(file_path)
        print(f"加载 {year}年第{week}周数据: {len(df)} 行, {len(df.columns)} 列")
        return df
    
    def load_year_data(self, year, weeks=None):
        """加载指定年度数据"""
        if year not in self.data_structure['available_years']:
            available_years = self.data_structure['available_years']
            raise ValueError(f"年度 {year} 不存在，可用年度: {available_years}")
        
        available_weeks = self.data_structure['year_week_mapping'][year]
        
        if weeks is None:
            weeks = available_weeks
        else:
            # 验证周次是否存在
            invalid_weeks = set(weeks) - set(available_weeks)
            if invalid_weeks:
                print(f"警告: 周次 {sorted(invalid_weeks)} 不存在，将跳过")
            weeks = [w for w in weeks if w in available_weeks]
        
        dataframes = []
        for week in sorted(weeks):
            file_path = self.data_structure['file_paths'][year][week]
            df = pd.read_csv(file_path)
            dataframes.append(df)
        
        result = pd.concat(dataframes, ignore_index=True)
        print(f"加载 {year}年数据: {len(weeks)} 个文件, 总计 {len(result)} 行")
        return result
    
    def load_all_data(self, years=None):
        """加载所有可用数据"""
        if years is None:
            years = self.data_structure['available_years']
        
        dataframes = []
        for year in sorted(years):
            if year in self.data_structure['available_years']:
                year_data = self.load_year_data(year)
                dataframes.append(year_data)
            else:
                print(f"警告: 年度 {year} 数据不存在，跳过")
        
        if not dataframes:
            raise ValueError("没有找到任何数据文件")
        
        result = pd.concat(dataframes, ignore_index=True)
        print(f"加载完成: {len(years)} 个年度, 总计 {len(result)} 行")
        return result
    
    def get_data_summary(self):
        """获取数据概览"""
        summary = {
            'total_years': len(self.data_structure['available_years']),
            'year_range': {
                'min': min(self.data_structure['available_years']) if self.data_structure['available_years'] else None,
                'max': max(self.data_structure['available_years']) if self.data_structure['available_years'] else None
            },
            'total_files': sum(len(weeks) for weeks in self.data_structure['year_week_mapping'].values()),
            'year_details': {}
        }
        
        for year, weeks in self.data_structure['year_week_mapping'].items():
            summary['year_details'][year] = {
                'week_count': len(weeks),
                'week_range': {'min': min(weeks), 'max': max(weeks)} if weeks else None,
                'missing_weeks': self._find_missing_weeks(weeks)
            }
        
        return summary
    
    def _find_missing_weeks(self, weeks):
        """查找缺失的周次"""
        if not weeks:
            return []
        
        min_week, max_week = min(weeks), max(weeks)
        expected = set(range(min_week, max_week + 1))
        actual = set(weeks)
        return sorted(list(expected - actual))

# 使用示例
loader = DataLoader('data4dash')

# 获取数据概览
summary = loader.get_data_summary()
print(f"数据概览: {summary['total_years']} 个年度, {summary['total_files']} 个文件")

# 加载单个文件
df_single = loader.load_single_file(2024, 28)

# 加载指定年度数据
df_2024 = loader.load_year_data(2024)

# 加载指定周次范围
df_recent = loader.load_year_data(2024, weeks=[34, 35, 36])

# 加载所有数据
df_all = loader.load_all_data()
```

#### 4.1.2 传统加载方式（向后兼容）

```python
# 单文件加载
df = pd.read_csv('data4dash/data-2025/2025保单第28周变动成本明细表.csv')
print(f"数据规模: {len(df)} 行, {len(df.columns)} 列")

# 批量加载（需要手动指定路径）
files_2025 = glob.glob('data4dash/data-2025/*.csv')
df_2025 = pd.concat([pd.read_csv(f) for f in files_2025], ignore_index=True)

# 加载所有年度数据
all_files = glob.glob('data4dash/data-*/*.csv')
df_all = pd.concat([pd.read_csv(f) for f in all_files], ignore_index=True)

print(f"总数据量: {len(df_all)} 行")
```

#### 4.1.3 内存优化加载
```python
# 指定数据类型以优化内存使用
dtype_dict = {
    'policy_start_year': 'int16',
    'week_number': 'int8',
    'policy_count': 'int32',
    'claim_case_count': 'int32',
    'is_new_energy_vehicle': 'bool',
    'is_transferred_vehicle': 'bool'
}

df = pd.read_csv('data4dash/data-2024/2024保单第28周变动成本明细表.csv', dtype=dtype_dict)
```

### 4.2 数据质量检查

```python
def data_quality_check(df):
    """数据质量检查函数"""
    report = {
        'total_records': len(df),
        'fields_count': len(df.columns),
        'completeness': {},
        'data_ranges': {}
    }
    
    # 完整性检查
    for col in df.columns:
        report['completeness'][col] = {
            'non_null_count': df[col].notna().sum(),
            'completeness_rate': df[col].notna().mean()
        }
    
    # 关键字段数值范围检查
    numeric_fields = [
        'signed_premium_yuan', 'matured_premium_yuan',
         'policy_count', 'claim_case_count', 'reported_claim_payment_yuan'
    ]
    
    for field in numeric_fields:
        if field in df.columns:
            report['data_ranges'][field] = {
                'min': df[field].min(),
                'max': df[field].max(),
                'mean': df[field].mean(),
                'zero_count': (df[field] == 0).sum()
            }
    
    return report
```

### 4.3 灵活筛选函数

```python
def filter_data(df, **kwargs):
    """
    灵活的数据筛选函数
    
    参数示例:
        year=2025, branch='成都', insurance_type='商业保险'
        week_range=(28, 32), new_energy=True
    """
    result = df.copy()
    
    if 'year' in kwargs:
        result = result[result['policy_start_year'] == kwargs['year']]
    
    if 'branch' in kwargs:
        result = result[result['chengdu_branch'] == kwargs['branch']]
    
    if 'insurance_type' in kwargs:
        result = result[result['insurance_type'] == kwargs['insurance_type']]
    
    if 'week_range' in kwargs:
        start_week, end_week = kwargs['week_range']
        result = result[result['week_number'].between(start_week, end_week)]
    
    if 'new_energy' in kwargs:
        result = result[result['is_new_energy_vehicle'] == kwargs['new_energy']]
    
    return result

# 使用示例
filtered_data = filter_data(
    df, 
    year=2025, 
    branch='成都', 
    week_range=(28, 32),
    insurance_type='商业保险'
)
```

---

## 5. 典型应用场景

### 5.1 趋势分析

#### 5.1.1 周度保费趋势
```python
# 按周统计保费趋势
weekly_trend = aggregate_and_calculate_metrics(df, ['week_number'])
weekly_trend = weekly_trend.sort_values('week_number')

# 可视化趋势
import matplotlib.pyplot as plt
plt.figure(figsize=(12, 6))
plt.plot(weekly_trend['week_number'], weekly_trend['signed_premium_yuan'] / 10000)
plt.title('周度保费趋势（万元）')
plt.xlabel('周数')
plt.ylabel('保费（万元）')
plt.grid(True)
plt.show()
```

#### 5.1.2 赔付率趋势分析
```python
# 按周分析赔付率变化
weekly_loss_ratio = aggregate_and_calculate_metrics(
    df, 
    ['week_number', 'insurance_type']
)

# 分险种展示趋势
for insurance_type in weekly_loss_ratio['insurance_type'].unique():
    data = weekly_loss_ratio[weekly_loss_ratio['insurance_type'] == insurance_type]
    plt.plot(data['week_number'], data['expired_loss_ratio_percent'], 
             label=insurance_type, marker='o')

plt.title('分险种赔付率趋势')
plt.xlabel('周数')
plt.ylabel('赔付率（%）')
plt.legend()
plt.grid(True)
plt.show()
```

### 5.2 对比分析

#### 5.2.1 机构经营对比
```python
# 按机构对比经营指标
org_comparison = aggregate_and_calculate_metrics(
    df,
    ['chengdu_branch', 'insurance_type']
)

# 计算机构排名
org_ranking = org_comparison.groupby('chengdu_branch').agg({
    'signed_premium_yuan': 'sum',
    'policy_count': 'sum',
    'reported_claim_payment_yuan': 'sum'
}).reset_index()

org_ranking = aggregate_and_calculate_metrics(org_ranking, [])
org_ranking = org_ranking.sort_values('signed_premium_yuan', ascending=False)

print("机构经营排名:")
print(org_ranking[[
    'chengdu_branch', 
    'signed_premium_yuan',
    'average_premium_per_policy_yuan', 
    'expired_loss_ratio_percent'
]])
```

#### 5.2.2 客户类型分析
```python
# 按客户类型分析业务结构
customer_analysis = aggregate_and_calculate_metrics(
    df,
    ['customer_category_3', 'insurance_type']
)

# 计算各客户类型的业务贡献度
customer_analysis['premium_contribution'] = (
    customer_analysis['signed_premium_yuan'] / 
    customer_analysis['signed_premium_yuan'].sum() * 100
)

# 按保费贡献度排序
customer_ranking = customer_analysis.sort_values(
    'premium_contribution', 
    ascending=False
)

print("客户类型业务贡献度:")
print(customer_ranking[[
    'customer_category_3',
    'insurance_type', 
    'signed_premium_yuan',
    'premium_contribution',
    'expired_loss_ratio_percent'
]].head(10))
```

### 5.3 风险分析

#### 5.3.1 风险等级出险分析
```python
# 按风险等级分析出险表现
risk_analysis = df.groupby(['large_truck_score', 'insurance_type']).agg({
    'policy_count': 'sum',
    'claim_case_count': 'sum',
    'reported_claim_payment_yuan': 'sum',
    'signed_premium_yuan': 'sum'
}).reset_index()

risk_analysis = aggregate_and_calculate_metrics(risk_analysis, [])

# 过滤有效数据
risk_analysis = risk_analysis[
    (risk_analysis['large_truck_score'].notna()) & 
    (risk_analysis['policy_count'] > 100)  # 样本量过滤
]

print("风险等级出险分析:")
print(risk_analysis[[
    'large_truck_score', 
    'insurance_type',
    'policy_count',
    'claim_frequency_percent', 
    'expired_loss_ratio_percent'
]].sort_values('claim_frequency_percent', ascending=False))
```

#### 5.3.2 新能源车风险对比
```python
# 新能源车 vs 传统车风险对比
energy_comparison = aggregate_and_calculate_metrics(
    df,
    ['is_new_energy_vehicle', 'insurance_type']
)

# 添加车辆类型标签
energy_comparison['vehicle_type'] = energy_comparison['is_new_energy_vehicle'].map({
    True: '新能源车',
    False: '传统车'
})

print("新能源车风险对比:")
print(energy_comparison[[
    'vehicle_type',
    'insurance_type',
    'policy_count',
    'average_premium_per_policy_yuan',
    'claim_frequency_percent',
    'expired_loss_ratio_percent'
]])
```

### 5.4 盈利能力分析

```python
# 综合盈利能力分析
profitability_analysis = aggregate_and_calculate_metrics(
    df,
    ['chengdu_branch', 'insurance_type', 'customer_category_3']
)

# 计算边际贡献（简化版，假设费用率为12%）
profitability_analysis['estimated_expense_ratio'] = 12.0  # %
profitability_analysis['estimated_variable_cost_ratio'] = (
    profitability_analysis['expired_loss_ratio_percent'] + 
    profitability_analysis['estimated_expense_ratio']
)
profitability_analysis['estimated_margin_ratio'] = (
    100 - profitability_analysis['estimated_variable_cost_ratio']
)

# 筛选盈利能力较好的业务
profitable_business = profitability_analysis[
    (profitability_analysis['estimated_margin_ratio'] > 20) &
    (profitability_analysis['policy_count'] > 50)
].sort_values('estimated_margin_ratio', ascending=False)

print("高盈利业务分析:")
print(profitable_business[[
    'chengdu_branch',
    'insurance_type', 
    'customer_category_3',
    'policy_count',
    'signed_premium_yuan',
    'expired_loss_ratio_percent',
    'estimated_margin_ratio'
]].head(10))
```

---

## 6. 技术注意事项

### 6.1 数据类型处理

```python
# 日期字段转换
df['snapshot_date'] = pd.to_datetime(df['snapshot_date'])

# 布尔字段确认
print("布尔字段唯一值:")
print(f"is_new_energy_vehicle: {df['is_new_energy_vehicle'].unique()}")
print(f"is_transferred_vehicle: {df['is_transferred_vehicle'].unique()}")

# 分类字段编码（如需要）
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['business_type_encoded'] = le.fit_transform(df['business_type_category'])
```

### 6.2 空值处理策略

```python
# 检查空值情况
null_summary = df.isnull().sum()
print("空值统计:")
print(null_summary[null_summary > 0])

# 风险评级字段空值处理
risk_fields = [
    'vehicle_insurance_grade', 
    'highway_risk_grade', 
    'large_truck_score', 
    'small_truck_score'
]

for field in risk_fields:
    df[field] = df[field].fillna('未评级')
    
# 商业险字段空值处理（交强险数据中为0是正常的）
df['commercial_premium_before_discount_yuan'] = df['commercial_premium_before_discount_yuan'].fillna(0)
```

### 6.3 性能优化建议 <mcreference link="https://www.askwisdom.ai/ai-data-preparation" index="3">3</mcreference>

```python
# 1. 使用分块处理大数据
def process_large_dataset(file_path, chunk_size=10000):
    results = []
    for chunk in pd.read_csv(file_path, chunksize=chunk_size):
        # 处理每个数据块
        processed_chunk = aggregate_and_calculate_metrics(chunk, [])
        results.append(processed_chunk)
    
    return pd.concat(results, ignore_index=True)

# 2. 使用并行处理
from multiprocessing import Pool

def parallel_file_processing(file_list):
    with Pool() as pool:
        results = pool.map(pd.read_csv, file_list)
    return pd.concat(results, ignore_index=True)

# 3. 缓存计算结果
import pickle

def cache_calculation_result(df, cache_file):
    result = aggregate_and_calculate_metrics(df, [])
    with open(cache_file, 'wb') as f:
        pickle.dump(result, f)
    return result

def load_cached_result(cache_file):
    with open(cache_file, 'rb') as f:
        return pickle.load(f)
```

---

## 7. 扩展功能与未来发展

### 7.1 数据更新机制与动态适应

#### 7.1.1 自动化数据管理

```python
import os
import json
import shutil
from datetime import datetime
from pathlib import Path

class DataManager:
    """
    数据管理器，处理动态数据更新和维护
    """
    
    def __init__(self, base_path='data4dash'):
        self.base_path = Path(base_path)
        self.metadata_path = self.base_path / 'metadata'
        self.ensure_structure()
    
    def ensure_structure(self):
        """确保基础目录结构存在"""
        self.base_path.mkdir(exist_ok=True)
        self.metadata_path.mkdir(exist_ok=True)
        
        # 创建元数据文件
        for metadata_file in ['available_years.json', 'available_weeks.json', 'data_catalog.json']:
            file_path = self.metadata_path / metadata_file
            if not file_path.exists():
                file_path.write_text('{}', encoding='utf-8')
    
    def add_new_data_file(self, file_path, year=None, week=None):
        """添加新的数据文件"""
        source_path = Path(file_path)
        
        if not source_path.exists():
            raise FileNotFoundError(f"源文件不存在: {file_path}")
        
        # 自动提取年度和周次信息
        if year is None or week is None:
            filename = source_path.name
            year_match = re.search(r'(\d{4})', filename)
            week_match = re.search(r'第(\d+)周', filename)
            
            if year_match:
                year = int(year_match.group(1))
            if week_match:
                week = int(week_match.group(1))
        
        if year is None or week is None:
            raise ValueError("无法从文件名提取年度或周次信息")
        
        # 创建目标目录
        target_dir = self.base_path / f'data-{year}'
        target_dir.mkdir(exist_ok=True)
        
        # 生成标准文件名
        target_filename = f'{year}保单第{week:02d}周变动成本明细表.csv'
        target_path = target_dir / target_filename
        
        # 复制文件
        shutil.copy2(source_path, target_path)
        
        # 更新元数据
        self.update_metadata()
        
        print(f"已添加数据文件: {target_path}")
        return target_path
    
    def update_metadata(self):
        """更新元数据信息"""
        loader = DataLoader(str(self.base_path))
        structure = loader.data_structure
        
        # 更新可用年度
        years_file = self.metadata_path / 'available_years.json'
        years_data = {
            'years': structure['available_years'],
            'last_updated': datetime.now().isoformat()
        }
        years_file.write_text(json.dumps(years_data, indent=2, ensure_ascii=False), encoding='utf-8')
        
        # 更新周次映射
        weeks_file = self.metadata_path / 'available_weeks.json'
        weeks_data = {
            'year_week_mapping': structure['year_week_mapping'],
            'last_updated': datetime.now().isoformat()
        }
        weeks_file.write_text(json.dumps(weeks_data, indent=2, ensure_ascii=False), encoding='utf-8')
        
        # 更新数据目录
        catalog_file = self.metadata_path / 'data_catalog.json'
        catalog_data = {
            'total_years': len(structure['available_years']),
            'total_files': sum(len(weeks) for weeks in structure['year_week_mapping'].values()),
            'file_paths': structure.get('file_paths', {}),
            'last_scan': datetime.now().isoformat()
        }
        catalog_file.write_text(json.dumps(catalog_data, indent=2, ensure_ascii=False), encoding='utf-8')
    
    def validate_data_integrity(self):
        """验证数据完整性"""
        issues = []
        loader = DataLoader(str(self.base_path))
        
        for year, weeks in loader.data_structure['year_week_mapping'].items():
            if not weeks:
                issues.append(f"年度 {year} 没有数据文件")
                continue
            
            # 检查缺失周次
            min_week, max_week = min(weeks), max(weeks)
            expected_weeks = set(range(min_week, max_week + 1))
            actual_weeks = set(weeks)
            missing_weeks = expected_weeks - actual_weeks
            
            if missing_weeks:
                issues.append(f"年度 {year} 缺失周次: {sorted(missing_weeks)}")
            
            # 检查文件完整性
            for week in weeks:
                try:
                    df = loader.load_single_file(year, week)
                    if len(df) == 0:
                        issues.append(f"{year}年第{week}周数据文件为空")
                except Exception as e:
                    issues.append(f"{year}年第{week}周数据文件损坏: {str(e)}")
        
        return issues
    
    def archive_old_data(self, years_to_archive):
        """归档历史数据"""
        archive_dir = self.base_path / 'archive'
        archive_dir.mkdir(exist_ok=True)
        
        for year in years_to_archive:
            source_dir = self.base_path / f'data-{year}'
            if source_dir.exists():
                target_dir = archive_dir / f'data-{year}'
                shutil.move(str(source_dir), str(target_dir))
                print(f"已归档 {year} 年度数据到 {target_dir}")
        
        self.update_metadata()

# 使用示例
manager = DataManager('data4dash')

# 添加新数据文件
manager.add_new_data_file('/path/to/new/data/2025保单第37周变动成本明细表.csv')

# 验证数据完整性
issues = manager.validate_data_integrity()
if issues:
    print("发现数据问题:")
    for issue in issues:
        print(f"  - {issue}")
else:
    print("数据完整性检查通过")

# 归档旧数据
# manager.archive_old_data([2022, 2023])
```

#### 7.1.2 核心特性

- **自动年度识别**: 系统自动识别新年度数据并创建对应文件夹
- **智能文件命名**: 自动标准化文件命名格式，确保一致性
- **历史数据扩展**: 支持向前扩展历史年度数据（如2023年及更早）
- **未来兼容性**: 支持2026年及未来年度数据的自动处理
- **增量更新**: 新的周次数据可直接添加到对应年度文件夹
- **105周覆盖**: 系统设计支持完整的105周数据周期管理
- **数据验证**: 自动检查新数据的字段完整性和格式一致性
- **元数据管理**: 实时维护数据目录和索引信息
- **归档机制**: 支持历史数据的自动归档和管理

#### 7.1.3 动态适应能力

- **结构自发现**: 无需预定义数据结构，自动适应新的年度和周次
- **缺失数据处理**: 智能识别和报告缺失的数据文件
- **错误恢复**: 提供数据验证和修复机制
- **扩展性设计**: 支持新业务场景和数据格式的扩展

### 7.2 AI模型集成建议

```python
# 特征工程示例
def create_features_for_ml(df):
    """为机器学习模型创建特征"""
    features = df.copy()
    
    # 时间特征
    features['quarter'] = (features['week_number'] - 1) // 13 + 1
    features['is_year_end'] = features['week_number'] > 48
    
    # 业务特征
    features['is_commercial'] = features['insurance_type'] == '商业保险'
    features['is_renewal'] = features['renewal_status'] == '续保'
    
    # 风险特征
    features['has_risk_grade'] = features['vehicle_insurance_grade'].notna()
    features['is_high_risk'] = features['large_truck_score'].isin(['D', 'E'])
    
    return features

# 预测模型示例框架
from sklearn.ensemble import RandomForestRegressor

def build_loss_ratio_prediction_model(df):
    """构建赔付率预测模型"""
    features = create_features_for_ml(df)
    
    # 选择特征列
    feature_columns = [
        'policy_start_year', 'week_number', 'quarter',
        'is_commercial', 'is_renewal', 'is_new_energy_vehicle',
        'has_risk_grade', 'is_high_risk'
    ]
    
    X = pd.get_dummies(features[feature_columns])
    y = features['expired_loss_ratio_percent']
    
    # 训练模型
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X, y)
    
    return model, X.columns
```

### 7.3 可视化集成

```python
# 与主流可视化库集成
import plotly.express as px
import plotly.graph_objects as go

def create_interactive_dashboard_data(df):
    """为交互式仪表板准备数据"""
    # 时间序列数据
    time_series = aggregate_and_calculate_metrics(
        df, ['week_number', 'insurance_type']
    )
    
    # 机构对比数据
    org_comparison = aggregate_and_calculate_metrics(
        df, ['chengdu_branch', 'third_level_organization']
    )
    
    # 风险分布数据
    risk_distribution = aggregate_and_calculate_metrics(
        df, ['vehicle_insurance_grade', 'customer_category_3']
    )
    
    return {
        'time_series': time_series,
        'org_comparison': org_comparison,
        'risk_distribution': risk_distribution
    }

# Plotly图表示例
def create_plotly_charts(dashboard_data):
    """创建Plotly交互图表"""
    # 时间趋势图
    fig_trend = px.line(
        dashboard_data['time_series'],
        x='week_number',
        y='signed_premium_yuan',
        color='insurance_type',
        title='保费趋势分析'
    )
    
    # 机构对比图
    fig_org = px.bar(
        dashboard_data['org_comparison'],
        x='third_level_organization',
        y='expired_loss_ratio_percent',
        color='chengdu_branch',
        title='机构赔付率对比'
    )
    
    return fig_trend, fig_org
```

---

## 8. 总结

### 8.1 核心优势

1. **数据结构清晰**: 17个筛选维度 + 9个绝对值字段 + 7个计算字段的分层设计
2. **计算准确可靠**: 专用的代码计算函数确保聚合场景下的计算准确性
3. **扩展性强**: 支持未来年度数据自动扩展和新业务场景适配
4. **AI友好**: 结构化数据格式便于AI系统解析和处理 <mcreference link="https://docs.kapa.ai/improving/writing-best-practices" index="1">1</mcreference>

### 8.2 应用价值

- **业务洞察**: 支持多维度的车险成本分析和风险评估
- **决策支持**: 为经营管理提供数据驱动的决策依据
- **效率提升**: 自动化的计算和分析流程提高工作效率
- **风险管控**: 实时的风险指标监控和预警机制

### 8.3 使用建议

1. **始终使用实时计算**: 避免直接聚合率值字段
2. **注意数据质量**: 定期检查数据完整性和一致性
3. **合理选择分析维度**: 根据业务需求选择合适的分组维度
4. **关注性能优化**: 大数据量时采用分块处理和缓存策略

---

**文档版本**: v1.0  
**最后更新**: 2025-01-27  
**适用系统**: 车险变动成本多维分析系统  
**技术支持**: Python代码计算 + Pandas + Python 3.8+  

---

*本文档整合了数据结构说明、字段分类清单、使用示例和AI开发指南，为车险变动成本分析系统的开发提供完整的数据基础和技术指导。*