# è½¦é™©å˜åŠ¨æˆæœ¬å¤šç»´åˆ†æç³»ç»Ÿ - æ•°æ®ç»¼åˆè¯´æ˜æ–‡æ¡£

## ğŸ“‹ æ–‡æ¡£æ¦‚è¦

### ä¸»è¦ç»„æˆéƒ¨åˆ†
1. **æ•°æ®æ¶æ„æ¦‚è¿°** - å®Œæ•´çš„æ•°æ®ç»“æ„å’Œæ–‡ä»¶ç»„ç»‡è¯´æ˜
2. **å­—æ®µåˆ†ç±»ä½“ç³»** - 17ä¸ªç­›é€‰ç»´åº¦ + 9ä¸ªç»å¯¹å€¼å­—æ®µ + 7ä¸ªè®¡ç®—å­—æ®µçš„è¯¦ç»†å®šä¹‰
3. **æŒ‡æ ‡è®¡ç®—æ–¹æ³•** - å„ç±»æŒ‡æ ‡çš„è®¡ç®—å…¬å¼å’Œä»£ç å®ç°ç¤ºä¾‹
4. **AIå¼€å‘æŒ‡å—** - é’ˆå¯¹AIç³»ç»Ÿçš„æ•°æ®åŠ è½½ã€å¤„ç†å’Œåˆ†ææœ€ä½³å®è·µ
5. **åº”ç”¨åœºæ™¯ç¤ºä¾‹** - å…¸å‹çš„ä¸šåŠ¡åˆ†æåœºæ™¯å’Œä»£ç å®ç°

### å…³é”®ä¿¡æ¯ç‚¹
- **æ•°æ®è§„æ¨¡**: æ¯å‘¨çº¦16,000è¡Œ Ã— 23ä¸ªå­˜å‚¨å­—æ®µï¼Œå½“å‰è¦†ç›–æœ€è¿‘2å¹´æ•°æ®
- **æ•°æ®å‘¨æœŸ**: è€ƒè™‘ä¿å•1å¹´æœŸé™å’Œå…¨å¹´åˆ†å¸ƒç”Ÿæ•ˆç‰¹ç‚¹ï¼Œå®Œæ•´è¿½è¸ªéœ€è¦†ç›–105å‘¨
- **æ ¸å¿ƒç‰¹æ€§**: ç»å¯¹å€¼å­˜å‚¨ + å®æ—¶è®¡ç®—ï¼Œç¡®ä¿èšåˆåˆ†æçš„å‡†ç¡®æ€§
- **æŠ€æœ¯æ ˆ**: Python + Pandas + ä¸“ç”¨è®¡ç®—å·¥å…·
- **åº”ç”¨ç›®æ ‡**: æ”¯æ’‘è½¦é™©å˜åŠ¨æˆæœ¬çš„å¤šç»´åº¦åˆ†æå’Œå¯è§†åŒ–

---

## 1. æ•°æ®æ¶æ„æ¦‚è¿°

### 1.1 æ•°æ®å­˜å‚¨æ¶æ„

#### 1.1.1 åŠ¨æ€ç›®å½•ç»“æ„

```
data4dash/                           # ä¸“é—¨ä¸ºä»ªè¡¨æ¿æä¾›çš„æ•°æ®
â”œâ”€â”€ data-{YYYY}/                    # å¹´åº¦æ•°æ®æ–‡ä»¶å¤¹ï¼ˆåŠ¨æ€åˆ›å»ºï¼‰
â”‚   â””â”€â”€ {YYYY}ä¿å•ç¬¬{WW}å‘¨å˜åŠ¨æˆæœ¬æ˜ç»†è¡¨.csv  # å‘¨æ¬¡æ•°æ®æ–‡ä»¶ï¼ˆæŒ‰éœ€ç”Ÿæˆï¼‰
â”œâ”€â”€ metadata/                       # å…ƒæ•°æ®ç®¡ç†
â”‚   â”œâ”€â”€ available_years.json        # å¯ç”¨å¹´åº¦åˆ—è¡¨
â”‚   â”œâ”€â”€ available_weeks.json        # å„å¹´åº¦å¯ç”¨å‘¨æ¬¡
â”‚   â””â”€â”€ data_catalog.json          # æ•°æ®ç›®å½•ç´¢å¼•
â””â”€â”€ archive/                        # å†å²æ•°æ®å½’æ¡£ï¼ˆå¯é€‰ï¼‰
    â””â”€â”€ data-{YYYY}/               # å½’æ¡£çš„å†å²å¹´åº¦æ•°æ®
```

#### 1.1.2 æ–‡ä»¶å‘½åè§„èŒƒ

**æ ‡å‡†æ ¼å¼**: `{å¹´åº¦}ä¿å•ç¬¬{å‘¨æ¬¡}å‘¨å˜åŠ¨æˆæœ¬æ˜ç»†è¡¨.csv`
- **å¹´åº¦**: 4ä½æ•°å­—ï¼Œå¦‚2024ã€2025ã€2026ç­‰
- **å‘¨æ¬¡**: 2ä½æ•°å­—ï¼ŒèŒƒå›´1-53ï¼Œå¦‚28ã€29ã€30ç­‰
- **ç¤ºä¾‹**: `2024ä¿å•ç¬¬28å‘¨å˜åŠ¨æˆæœ¬æ˜ç»†è¡¨.csv`

#### 1.1.3 å½“å‰æ•°æ®çŠ¶æ€ï¼ˆç¤ºä¾‹ï¼‰

```
data4dash/
â”œâ”€â”€ data-2024/                      # 2024å¹´åº¦æ•°æ®
â”‚   â”œâ”€â”€ 2024ä¿å•ç¬¬28å‘¨å˜åŠ¨æˆæœ¬æ˜ç»†è¡¨.csv
â”‚   â”œâ”€â”€ 2024ä¿å•ç¬¬29å‘¨å˜åŠ¨æˆæœ¬æ˜ç»†è¡¨.csv
â”‚   â”œâ”€â”€ 2024ä¿å•ç¬¬30å‘¨å˜åŠ¨æˆæœ¬æ˜ç»†è¡¨.csv
â”‚   â”œâ”€â”€ 2024ä¿å•ç¬¬31å‘¨å˜åŠ¨æˆæœ¬æ˜ç»†è¡¨.csv
â”‚   â”œâ”€â”€ 2024ä¿å•ç¬¬33å‘¨å˜åŠ¨æˆæœ¬æ˜ç»†è¡¨.csv  # æ³¨ï¼šç¬¬32å‘¨æ•°æ®ç¼ºå¤±
â”‚   â”œâ”€â”€ 2024ä¿å•ç¬¬34å‘¨å˜åŠ¨æˆæœ¬æ˜ç»†è¡¨.csv
â”‚   â”œâ”€â”€ 2024ä¿å•ç¬¬35å‘¨å˜åŠ¨æˆæœ¬æ˜ç»†è¡¨.csv
â”‚   â””â”€â”€ 2024ä¿å•ç¬¬36å‘¨å˜åŠ¨æˆæœ¬æ˜ç»†è¡¨.csv
â””â”€â”€ data-2025/                      # 2025å¹´åº¦æ•°æ®
    â”œâ”€â”€ 2025ä¿å•ç¬¬28å‘¨å˜åŠ¨æˆæœ¬æ˜ç»†è¡¨.csv
    â”œâ”€â”€ 2025ä¿å•ç¬¬29å‘¨å˜åŠ¨æˆæœ¬æ˜ç»†è¡¨.csv
    â”œâ”€â”€ 2025ä¿å•ç¬¬30å‘¨å˜åŠ¨æˆæœ¬æ˜ç»†è¡¨.csv
    â”œâ”€â”€ 2025ä¿å•ç¬¬31å‘¨å˜åŠ¨æˆæœ¬æ˜ç»†è¡¨.csv
    â”œâ”€â”€ 2025ä¿å•ç¬¬33å‘¨å˜åŠ¨æˆæœ¬æ˜ç»†è¡¨.csv
    â”œâ”€â”€ 2025ä¿å•ç¬¬34å‘¨å˜åŠ¨æˆæœ¬æ˜ç»†è¡¨.csv
    â”œâ”€â”€ 2025ä¿å•ç¬¬35å‘¨å˜åŠ¨æˆæœ¬æ˜ç»†è¡¨.csv
    â””â”€â”€ 2025ä¿å•ç¬¬36å‘¨å˜åŠ¨æˆæœ¬æ˜ç»†è¡¨.csv
```

#### 1.1.4 è‡ªåŠ¨å‘ç°æœºåˆ¶

```python
import os
import glob
import json
from pathlib import Path

def discover_data_structure(base_path='data4dash'):
    """
    è‡ªåŠ¨å‘ç°æ•°æ®ç»“æ„ï¼Œé€‚åº”åŠ¨æ€å˜åŒ–
    """
    structure = {
        'available_years': [],
        'year_week_mapping': {},
        'total_files': 0,
        'missing_weeks': {},
        'last_updated': None
    }
    
    # æ‰«æå¹´åº¦æ–‡ä»¶å¤¹
    year_dirs = glob.glob(f'{base_path}/data-*')
    
    for year_dir in sorted(year_dirs):
        year = int(os.path.basename(year_dir).split('-')[1])
        structure['available_years'].append(year)
        
        # æ‰«æå‘¨æ¬¡æ–‡ä»¶
        week_files = glob.glob(f'{year_dir}/*.csv')
        weeks = []
        
        for file_path in week_files:
            filename = os.path.basename(file_path)
            # æå–å‘¨æ¬¡ä¿¡æ¯
            week_match = re.search(r'ç¬¬(\d+)å‘¨', filename)
            if week_match:
                week = int(week_match.group(1))
                weeks.append(week)
        
        structure['year_week_mapping'][year] = sorted(weeks)
        structure['total_files'] += len(weeks)
        
        # æ£€æµ‹ç¼ºå¤±å‘¨æ¬¡
        if weeks:
            min_week, max_week = min(weeks), max(weeks)
            expected_weeks = set(range(min_week, max_week + 1))
            actual_weeks = set(weeks)
            missing = expected_weeks - actual_weeks
            if missing:
                structure['missing_weeks'][year] = sorted(list(missing))
    
    return structure

# ä½¿ç”¨ç¤ºä¾‹
data_structure = discover_data_structure()
print(f"å‘ç° {len(data_structure['available_years'])} ä¸ªå¹´åº¦çš„æ•°æ®")
print(f"æ€»è®¡ {data_structure['total_files']} ä¸ªæ•°æ®æ–‡ä»¶")
```

### 1.2 æ•°æ®è§„æ¨¡ç‰¹å¾

| ç»´åº¦ | æ•°å€¼ | è¯´æ˜ |
|------|------|------|
| **å½“å‰æ–‡ä»¶æ•°** | 16ä¸ª | æœ€è¿‘2å¹´ Ã— 8å‘¨ |
| **å•å‘¨æ•°æ®é‡** | ~16,000è¡Œ | æ¯æ–‡ä»¶çº¦16,000è¡Œ |
| **å­˜å‚¨å­—æ®µæ•°** | 23ä¸ª | 17ç»´åº¦ + 6ç»å¯¹å€¼ |
| **è®¡ç®—å­—æ®µæ•°** | 7ä¸ª | å®æ—¶è®¡ç®—è·å¾— |
| **ç†è®ºæ•°æ®å‘¨æœŸ** | 105å‘¨ | å®Œæ•´è¿½è¸ª1å¹´æœŸä¿å•æ‰€éœ€ |
| **å½“å‰æ—¶é—´è·¨åº¦** | æœ€è¿‘2å¹´ | ç¬¬28-36å‘¨ï¼ˆç¼º32å‘¨ï¼‰ |
| **åœ°åŸŸå±æ€§** | æˆéƒ½ã€ä¸­æ”¯ | ä¸‰çº§æœºæ„æ˜¯å¦åœ¨çœä¼š |
| **é™©ç§ç±»å‹** | å•†ä¸šé™©ã€äº¤å¼ºé™© | å•†ä¸šä¿é™©ã€å¼ºåˆ¶ä¿é™© |

#### æ•°æ®å‘¨æœŸè¯´æ˜
**ä¿å•æœŸé™ç‰¹æ€§**: è½¦é™©ä¿å•æœŸé™ä¸º1å¹´ï¼Œä½†ä¿å•ç”Ÿæ•ˆæ—¥æœŸåˆ†å¸ƒåœ¨å…¨å¹´ä»»ä½•æ—¶é—´
- 1æœˆ1æ—¥ç”Ÿæ•ˆä¿å• â†’ å½“å¹´12æœˆ31æ—¥åˆ°æœŸ
- 12æœˆ31æ—¥ç”Ÿæ•ˆä¿å• â†’ æ¬¡å¹´12æœˆ30æ—¥åˆ°æœŸ
- **å®Œæ•´è¿½è¸ª**: æŸå¹´åº¦ä¿å•æ•°æ®éœ€è¦†ç›–105å‘¨ï¼ˆçº¦2å¹´+1å‘¨ï¼‰
- **å½“å‰è¦†ç›–**: åŸºäºä¸šåŠ¡éœ€è¦é€‰æ‹©å…³é”®å‘¨æ¬¡è¿›è¡Œåˆ†æ

### 1.3 æ•°æ®é‡æ„ç‰¹ç‚¹ <mcreference link="https://docs.kapa.ai/improving/writing-best-practices" index="1">1</mcreference>

- **å­—æ®µåˆ†ç¦»**: ç­›é€‰ç»´åº¦ä¸ç»å¯¹å€¼å­—æ®µåˆ†ç¦»ï¼Œé¿å…ç‡å€¼ç›´æ¥èšåˆ
- **å•ä½ç»Ÿä¸€**: æ‰€æœ‰é‡‘é¢å­—æ®µç»Ÿä¸€ä¸º"å…ƒ"å•ä½
- **ç»“æ„ä¼˜åŒ–**: ç§»é™¤å†—ä½™å­—æ®µï¼Œä¿ç•™æ ¸å¿ƒä¸šåŠ¡æ•°æ®
- **è®¡ç®—å®‰å…¨**: é€šè¿‡å®æ—¶è®¡ç®—ç¡®ä¿èšåˆåœºæ™¯ä¸‹çš„å‡†ç¡®æ€§
- **æ‰©å±•è®¾è®¡**: æ”¯æŒå†å²æ•°æ®å¹´ä»½æ‰©å±•ï¼Œé€‚åº”é•¿æœŸæ•°æ®ç§¯ç´¯éœ€æ±‚

---

## 2. å­—æ®µåˆ†ç±»ä½“ç³»

### 2.1 ç­›é€‰ç»´åº¦å­—æ®µï¼ˆ17ä¸ªï¼‰

è¿™äº›å­—æ®µç”¨äºæ•°æ®ç­›é€‰ã€åˆ†ç»„å’Œå¤šç»´åˆ†æï¼š

#### 2.1.1 æ—¶é—´ç»´åº¦
| å­—æ®µå | æ•°æ®ç±»å‹ | é€‰é¡¹æ•° | ç¤ºä¾‹å€¼ | æè¿° |
|--------|----------|--------|--------|---------|
| `snapshot_date` | æ—¥æœŸ | - | 2025-07-13 | æ•°æ®å¿«ç…§æ—¥æœŸ |
| `policy_start_year` | æ•´æ•° | 2 | 2024, 2025 | ä¿å•èµ·æœŸå¹´åº¦ |
| `week_number` | æ•´æ•° | 8 | 28, 29, 30, 31, 33, 34, 35, 36 | å‘¨åºå· |

#### 2.1.2 æœºæ„ç»´åº¦
| å­—æ®µå | æ•°æ®ç±»å‹ | é€‰é¡¹æ•° | ä¸»è¦é€‰é¡¹ | æè¿° |
|--------|----------|--------|----------|---------|
| `chengdu_branch` | å­—ç¬¦ä¸² | 2 | æˆéƒ½, ä¸­æ”¯ | æœºæ„å±‚çº§ |
| `third_level_organization` | å­—ç¬¦ä¸² | 13 | ä¹å±±, å¤©åºœ, å®œå®¾, å¾·é˜³, æ–°éƒ½, æœ¬éƒ¨, æ­¦ä¾¯, æ³¸å·, è‡ªè´¡, èµ„é˜³, è¾¾å·, é’ç¾Š, é«˜æ–° | ä¸‰çº§æœºæ„ |

#### 2.1.3 ä¸šåŠ¡ç»´åº¦
| å­—æ®µå | æ•°æ®ç±»å‹ | é€‰é¡¹æ•° | ä¸»è¦é€‰é¡¹ | æè¿° |
|--------|----------|--------|----------|---------|
| `business_type_category` | å­—ç¬¦ä¸² | 16 | 10å¨ä»¥ä¸Š-æ™®è´§, éè¥ä¸šå®¢è½¦æ–°è½¦, è¥ä¸šè´§è½¦ç­‰ | ä¸šåŠ¡ç±»å‹åˆ†ç±» |
| `customer_category_3` | å­—ç¬¦ä¸² | 11 | è¥ä¸šè´§è½¦, éè¥ä¸šä¸ªäººå®¢è½¦, éè¥ä¸šä¼ä¸šå®¢è½¦ç­‰ | å®¢æˆ·ä¸‰çº§åˆ†ç±» |
| `insurance_type` | å­—ç¬¦ä¸² | 2 | å•†ä¸šä¿é™©, äº¤å¼ºé™© | é™©ç§ç±»å‹ |
| `coverage_type` | å­—ç¬¦ä¸² | 3 | ä¸»å…¨, äº¤ä¸‰, å•äº¤ | é™©åˆ«ç»„åˆ |
| `renewal_status` | å­—ç¬¦ä¸² | 3 | æ–°ä¿, ç»­ä¿, è½¬ä¿ | æ–°ç»­è½¬çŠ¶æ€ |
| `terminal_source` | å­—ç¬¦ä¸² | 7 | 0101æŸœé¢, 0105å¾®ä¿¡, 0106ç§»åŠ¨å±•ä¸šç­‰ | æŠ•ä¿ç»ˆç«¯æ¥æº |

#### 2.1.4 è½¦è¾†å±æ€§ç»´åº¦
| å­—æ®µå | æ•°æ®ç±»å‹ | é€‰é¡¹æ•° | ä¸»è¦é€‰é¡¹ | æè¿° |
|--------|----------|--------|----------|---------|
| `is_new_energy_vehicle` | å¸ƒå°” | 2 | True, False | æ˜¯å¦æ–°èƒ½æºè½¦ |
| `is_transferred_vehicle` | å¸ƒå°” | 2 | True, False | æ˜¯å¦è¿‡æˆ·è½¦ |

#### 2.1.5 é£é™©è¯„çº§ç»´åº¦
| å­—æ®µå | æ•°æ®ç±»å‹ | é€‰é¡¹æ•° | ä¸»è¦é€‰é¡¹ | æè¿° |
|--------|----------|--------|----------|---------|
| `vehicle_insurance_grade` | å­—ç¬¦ä¸² | 8 | A, B, C, D, E, F, G, X | è½¦é™©åˆ†ç­‰çº§ |
| `highway_risk_grade` | å­—ç¬¦ä¸² | 6 | A, B, C, D, E, X | é«˜é€Ÿé£é™©ç­‰çº§ |
| `large_truck_score` | å­—ç¬¦ä¸² | 6 | A, B, C, D, E, X | å¤§è´§è½¦è¯„åˆ† |
| `small_truck_score` | å­—ç¬¦ä¸² | 6 | A, B, C, D, E, X | å°è´§è½¦è¯„åˆ† |

### 2.2 ç»å¯¹å€¼å­—æ®µï¼ˆ9ä¸ªï¼‰

è¿™äº›å­—æ®µå­˜å‚¨ç»å¯¹å€¼æ•°æ®ï¼Œç”¨äºèšåˆè®¡ç®—ï¼š

| å­—æ®µå | æ•°æ®ç±»å‹ | å•ä½ | ç¤ºä¾‹å€¼ | æè¿° |
|--------|----------|------|--------|---------|
| `signed_premium_yuan` | æµ®ç‚¹æ•° | å…ƒ | 6762.26 | ç­¾å•ä¿è´¹ |
| `matured_premium_yuan` | æµ®ç‚¹æ•° | å…ƒ | 2617.05 | æ»¡æœŸä¿è´¹ |
| `commercial_premium_before_discount_yuan` | æµ®ç‚¹æ•° | å…ƒ | 0.0 | å•†ä¸šé™©æŠ˜å‰ä¿è´¹ |
| `policy_count` | æ•´æ•° | ä»¶ | 1 | ä¿å•ä»¶æ•° |
| `claim_case_count` | æ•´æ•° | ä»¶ | 0 | èµ”æ¡ˆä»¶æ•° |
| `reported_claim_payment_yuan` | æµ®ç‚¹æ•° | å…ƒ | 0.0 | å·²æŠ¥å‘Šèµ”æ¬¾ |
| `expense_amount_yuan` | æµ®ç‚¹æ•° | å…ƒ | 202.87 | è´¹ç”¨é‡‘é¢ |
| `premium_plan_yuan` | æµ®ç‚¹æ•° | å…ƒ | 8000.0 | ä¿è´¹è®¡åˆ’ |
| `marginal_contribution_amount_yuan` | æµ®ç‚¹æ•° | å…ƒ | 1500.0 | æ»¡æœŸè¾¹é™…è´¡çŒ®é¢ï¼ˆéç»ˆæï¼‰ |

### 2.3 è®¡ç®—å­—æ®µï¼ˆ7ä¸ªï¼‰

è¿™äº›å­—æ®µé€šè¿‡å®æ—¶è®¡ç®—è·å¾—ï¼Œä¸å­˜å‚¨åœ¨æ–‡ä»¶ä¸­ï¼š

| å­—æ®µå | è®¡ç®—å…¬å¼ | å•ä½ | æè¿° |
|--------|----------|------|---------|
| `average_premium_per_policy_yuan` | signed_premium_yuan Ã· policy_count | å…ƒ/ä»¶ | å•å‡ä¿è´¹ |
| `claim_frequency_percent` | (claim_case_count Ã· policy_count) Ã— 100 | % | æ»¡æœŸå‡ºé™©ç‡ |
| `average_claim_payment_yuan` | reported_claim_payment_yuan Ã· claim_case_count | å…ƒ/ä»¶ | æ¡ˆå‡èµ”æ¬¾ |
| expired_loss_ratio_percent | (reported_claim_payment_yuan Ã· matured_premium_yuan) Ã— 100 | % | æ»¡æœŸèµ”ä»˜ç‡ |
| `expense_ratio_percent` | (expense_amount_yuan Ã· signed_premium_yuan) Ã— 100 | % | è´¹ç”¨ç‡ |
| `variable_cost_ratio_percent` | ((expense_amount_yuan + reported_claim_payment_yuan) Ã· signed_premium_yuan) Ã— 100 | % | å˜åŠ¨æˆæœ¬ç‡ |
| `commercial_auto_underwriting_factor` | signed_premium_yuan Ã· commercial_premium_before_discount_yuan | ç³»æ•° | å•†ä¸šé™©è‡ªä¸»å®šä»·ç³»æ•° |

---

## 3. æŒ‡æ ‡è®¡ç®—æ–¹æ³•

### 3.1 åŸºç¡€è®¡ç®—åŸåˆ™

ä¸ºç¡®ä¿èšåˆåˆ†æçš„å‡†ç¡®æ€§ï¼Œæ‰€æœ‰æŒ‡æ ‡è®¡ç®—éƒ½éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š

```python
import pandas as pd
import numpy as np

def calculate_insurance_metrics(df):
    """
    è®¡ç®—è½¦é™©æ ¸å¿ƒæŒ‡æ ‡
    
    å‚æ•°:
    df: DataFrame - åŒ…å«åŸºç¡€æ•°æ®çš„DataFrame
    
    è¿”å›:
    DataFrame - åŒ…å«æ‰€æœ‰è®¡ç®—æŒ‡æ ‡çš„DataFrame
    """
    
    # åˆ›å»ºç»“æœDataFrameå‰¯æœ¬
    result_df = df.copy()
    
    # 1. åŸºç¡€å‡å€¼æŒ‡æ ‡
    result_df['average_premium_per_policy_yuan'] = (
        result_df['signed_premium_yuan'] / result_df['policy_count']
    )
    
    # å¤„ç†é™¤é›¶æƒ…å†µ
    result_df['average_claim_payment_yuan'] = np.where(
        result_df['claim_case_count'] > 0,
        result_df['reported_claim_payment_yuan'] / result_df['claim_case_count'],
        0
    )
    
    # 2. æ¯”ç‡æŒ‡æ ‡ï¼ˆç™¾åˆ†æ¯”ï¼‰
    result_df['expense_ratio_percent'] = (
        result_df['expense_amount_yuan'] / result_df['signed_premium_yuan'] * 100
    )
    
    result_df['maturity_ratio_percent'] = (
        result_df['matured_premium_yuan'] / result_df['signed_premium_yuan'] * 100
    )
    
    result_df['claim_frequency_percent'] = (
        result_df['claim_case_count'] / result_df['policy_count'] * 
        result_df['matured_premium_yuan'] / result_df['signed_premium_yuan'] * 100
    )
    
    result_df['matured_loss_ratio_percent'] = np.where(
        result_df['matured_premium_yuan'] > 0,
        result_df['reported_claim_payment_yuan'] / result_df['matured_premium_yuan'] * 100,
        0
    )
    
    result_df['variable_cost_ratio_percent'] = (
        result_df['expense_amount_yuan'] / result_df['signed_premium_yuan'] + 
        result_df['reported_claim_payment_yuan'] / result_df['matured_premium_yuan']
    ) * 100
    
    result_df['marginal_contribution_ratio_percent'] = (
        result_df['marginal_contribution_amount_yuan'] / result_df['matured_premium_yuan'] * 100
    )
    
    result_df['premium_time_progress_achievement_rate_percent'] = (
        result_df['signed_premium_yuan'] / result_df['premium_plan_yuan'] * 100
    )
    
    # 3. ç³»æ•°æŒ‡æ ‡
    result_df['commercial_auto_underwriting_factor'] = np.where(
        (result_df['insurance_type'] == 'å•†ä¸šä¿é™©') & 
        (result_df['commercial_premium_before_discount_yuan'] > 0),
        result_df['signed_premium_yuan'] / result_df['commercial_premium_before_discount_yuan'],
        np.nan
    )
    
    return result_df
```

### 3.2 æ ¸å¿ƒè®¡ç®—å…¬å¼è¯¦è§£

#### 3.2.1 åŸºç¡€æŒ‡æ ‡å…¬å¼
```python
# å•å‡ä¿è´¹
average_premium_per_policy = signed_premium_yuan / policy_count

# æ»¡æœŸå‡ºé™©ç‡ï¼ˆåŒ…å«æ»¡æœŸç‡ä¿®æ­£ï¼‰
claim_frequency_percent = (claim_case_count / policy_count) * (matured_premium_yuan / signed_premium_yuan) * 100

# æ¡ˆå‡èµ”æ¬¾
average_claim_payment = reported_claim_payment_yuan / claim_case_count

# æ»¡æœŸèµ”ä»˜ç‡
matured_loss_ratio_percent = (reported_claim_payment_yuan / matured_premium_yuan) * 100
```

#### 3.2.2 å¤åˆæŒ‡æ ‡å…¬å¼
```python
# è´¹ç”¨ç‡
expense_ratio_percent = (expense_amount_yuan / signed_premium_yuan) * 100

# å˜åŠ¨æˆæœ¬ç‡
variable_cost_ratio_percent = (
    (expense_amount_yuan / signed_premium_yuan) + 
    (reported_claim_payment_yuan / matured_premium_yuan)
) * 100

# è¾¹é™…è´¡çŒ®ç‡
marginal_contribution_ratio_percent = (marginal_contribution_amount_yuan / matured_premium_yuan) * 100

# å•†ä¸šé™©è‡ªä¸»å®šä»·ç³»æ•°ï¼ˆä»…é€‚ç”¨äºå•†ä¸šé™©ï¼‰
commercial_auto_underwriting_factor = signed_premium_yuan / commercial_premium_before_discount_yuan
```

#### 3.2.3 å®Œæ•´è®¡ç®—ç¤ºä¾‹
```python
def calculate_single_record_metrics(row):
    """
    è®¡ç®—å•æ¡è®°å½•çš„æ‰€æœ‰æŒ‡æ ‡
    """
    metrics = {}
    
    # åŸºç¡€æŒ‡æ ‡
    metrics['average_premium_per_policy_yuan'] = row['signed_premium_yuan'] / row['policy_count']
    
    if row['claim_case_count'] > 0:
        metrics['average_claim_payment_yuan'] = row['reported_claim_payment_yuan'] / row['claim_case_count']
    else:
        metrics['average_claim_payment_yuan'] = 0
    
    # æ¯”ç‡æŒ‡æ ‡
    metrics['expense_ratio_percent'] = (row['expense_amount_yuan'] / row['signed_premium_yuan']) * 100
    metrics['maturity_ratio_percent'] = (row['matured_premium_yuan'] / row['signed_premium_yuan']) * 100
    
    # æ»¡æœŸå‡ºé™©ç‡ï¼ˆåŒ…å«æ»¡æœŸç‡ä¿®æ­£ï¼‰
    metrics['claim_frequency_percent'] = (
        (row['claim_case_count'] / row['policy_count']) * 
        (row['matured_premium_yuan'] / row['signed_premium_yuan']) * 100
    )
    
    if row['matured_premium_yuan'] > 0:
        metrics['matured_loss_ratio_percent'] = (row['reported_claim_payment_yuan'] / row['matured_premium_yuan']) * 100
    else:
        metrics['matured_loss_ratio_percent'] = 0
    
    # å˜åŠ¨æˆæœ¬ç‡
    metrics['variable_cost_ratio_percent'] = (
        (row['expense_amount_yuan'] / row['signed_premium_yuan']) + 
        (row['reported_claim_payment_yuan'] / row['matured_premium_yuan'])
    ) * 100
    
    # è¾¹é™…è´¡çŒ®ç‡
    if row['matured_premium_yuan'] > 0:
        metrics['marginal_contribution_ratio_percent'] = (row['marginal_contribution_amount_yuan'] / row['matured_premium_yuan']) * 100
    else:
        metrics['marginal_contribution_ratio_percent'] = 0
    
    # ä¿è´¹æ—¶é—´è¿›åº¦è¾¾æˆç‡
    if row['premium_plan_yuan'] > 0:
        metrics['premium_time_progress_achievement_rate_percent'] = (row['signed_premium_yuan'] / row['premium_plan_yuan']) * 100
    else:
        metrics['premium_time_progress_achievement_rate_percent'] = 0
    
    # å•†ä¸šé™©è‡ªä¸»å®šä»·ç³»æ•°
    if row['insurance_type'] == 'å•†ä¸šä¿é™©' and row['commercial_premium_before_discount_yuan'] > 0:
        metrics['commercial_auto_underwriting_factor'] = row['signed_premium_yuan'] / row['commercial_premium_before_discount_yuan']
    else:
        metrics['commercial_auto_underwriting_factor'] = None
    
    return metrics
```

### 3.3 èšåˆè®¡ç®—æœ€ä½³å®è·µ

**é‡è¦åŸåˆ™**: å…ˆèšåˆç»å¯¹å€¼ï¼Œå†è®¡ç®—æ¯”ç‡

```python
def aggregate_and_calculate(df, group_by_fields):
    """
    æ­£ç¡®çš„èšåˆè®¡ç®—æ–¹å¼
    
    å‚æ•°:
    df: DataFrame - åŸå§‹æ•°æ®
    group_by_fields: list - åˆ†ç»„å­—æ®µåˆ—è¡¨
    
    è¿”å›:
    DataFrame - èšåˆåçš„ç»“æœï¼ŒåŒ…å«é‡æ–°è®¡ç®—çš„æŒ‡æ ‡
    """
    
    # 1. å…ˆèšåˆç»å¯¹å€¼å­—æ®µ
    agg_dict = {
        'signed_premium_yuan': 'sum',
        'matured_premium_yuan': 'sum', 
        'commercial_premium_before_discount_yuan': 'sum',
        'policy_count': 'sum',
        'claim_case_count': 'sum',
        'reported_claim_payment_yuan': 'sum',
        'expense_amount_yuan': 'sum',
        'premium_plan_yuan': 'sum',
        'marginal_contribution_amount_yuan': 'sum'
    }
    
    agg_result = df.groupby(group_by_fields).agg(agg_dict).reset_index()
    
    # 2. åŸºäºèšåˆåçš„ç»å¯¹å€¼é‡æ–°è®¡ç®—æ‰€æœ‰æŒ‡æ ‡
    agg_result = calculate_insurance_metrics(agg_result)
    
    return agg_result

# âœ… æ­£ç¡®çš„ä½¿ç”¨æ–¹å¼
grouped_data = aggregate_and_calculate(df, ['chengdu_branch', 'insurance_type'])

# âŒ é”™è¯¯çš„æ–¹å¼ï¼ˆç›´æ¥èšåˆæ¯”ç‡ä¼šå¯¼è‡´è®¡ç®—é”™è¯¯ï¼‰
# wrong_result = df.groupby(['chengdu_branch', 'insurance_type'])['matured_loss_ratio_percent'].mean()
```

### 3.2 æ ¸å¿ƒè®¡ç®—å…¬å¼

#### 3.2.1 åŸºç¡€æŒ‡æ ‡å…¬å¼
```python
# å•å‡ä¿è´¹
average_premium_per_policy = signed_premium_yuan / policy_count

# æ»¡æœŸå‡ºé™©ç‡
claim_frequency_percent = (claim_case_count / policy_count) * 100

# æ¡ˆå‡èµ”æ¬¾
average_claim_payment = reported_claim_payment_yuan / claim_case_count

# æ»¡æœŸèµ”ä»˜ç‡
expired_loss_ratio_percent = (reported_claim_payment_yuan / matured_premium_yuan) * 100
```

#### 3.2.2 å¤åˆæŒ‡æ ‡å…¬å¼
```python
# è´¹ç”¨ç‡ï¼ˆéœ€è¦è´¹ç”¨é‡‘é¢å­—æ®µï¼‰
expense_ratio_percent = (expense_amount_yuan / signed_premium_yuan) * 100

# å˜åŠ¨æˆæœ¬ç‡
variable_cost_ratio_percent = ((expense_amount_yuan + reported_claim_payment_yuan) / signed_premium_yuan) * 100

# å•†ä¸šé™©è‡ªä¸»å®šä»·ç³»æ•°
commercial_auto_underwriting_factor = signed_premium_yuan / commercial_premium_before_discount_yuan
```

### 3.3 èšåˆè®¡ç®—æœ€ä½³å®è·µ <mcreference link="https://www.askwisdom.ai/ai-data-preparation" index="3">3</mcreference>

**é‡è¦åŸåˆ™**: å…ˆèšåˆç»å¯¹å€¼ï¼Œå†è®¡ç®—æ¯”ç‡

```python
def aggregate_and_calculate_metrics(df, group_by_fields):
    """
    æ­£ç¡®çš„èšåˆè®¡ç®—æ–¹æ³•
    """
    # 1. å…ˆèšåˆç»å¯¹å€¼å­—æ®µ
    agg_result = df.groupby(group_by_fields).agg({
        'signed_premium_yuan': 'sum',
        'matured_premium_yuan': 'sum',
        'reported_claim_payment_yuan': 'sum',
        'expense_amount_yuan': 'sum',
        'marginal_contribution_amount_yuan': 'sum',
        'premium_plan_yuan': 'sum',
        'commercial_premium_before_discount_yuan': 'sum',
        'policy_count': 'sum',
        'claim_case_count': 'sum'
    }).reset_index()
    
    # 2. åŸºäºèšåˆåçš„ç»å¯¹å€¼é‡æ–°è®¡ç®—æ¯”ç‡
    agg_result['expired_loss_ratio_percent'] = (
        agg_result['reported_claim_payment_yuan'] / 
        agg_result['matured_premium_yuan'] * 100
    )
    
    agg_result['expense_ratio_percent'] = (
        agg_result['expense_amount_yuan'] / 
        agg_result['signed_premium_yuan'] * 100
    )
    
    agg_result['claim_frequency_percent'] = (
        (agg_result['claim_case_count'] / agg_result['policy_count']) * 
        (agg_result['matured_premium_yuan'] / agg_result['signed_premium_yuan']) * 100
    )
    
    agg_result['average_premium_per_policy_yuan'] = (
        agg_result['signed_premium_yuan'] / agg_result['policy_count']
    )
    
    # å¤„ç†é™¤é›¶æƒ…å†µ
    agg_result['average_claim_payment_yuan'] = np.where(
        agg_result['claim_case_count'] > 0,
        agg_result['reported_claim_payment_yuan'] / agg_result['claim_case_count'],
        0
    )
    
    return agg_result

# âŒ é”™è¯¯çš„èšåˆæ–¹å¼ï¼ˆç›´æ¥èšåˆæ¯”ç‡ï¼‰
def wrong_aggregation(df, group_by_fields):
    """
    è¿™ç§æ–¹å¼ä¼šå¯¼è‡´è®¡ç®—é”™è¯¯ï¼Œä¸è¦ä½¿ç”¨
    """
    return df.groupby(group_by_fields)['expired_loss_ratio_percent'].mean()
```

---

## 4. AIå¼€å‘æŒ‡å—

### 4.1 æ•°æ®åŠ è½½ç­–ç•¥ <mcreference link="https://docs.kapa.ai/improving/writing-best-practices" index="1">1</mcreference>

#### 4.1.1 æ™ºèƒ½æ•°æ®å‘ç°ä¸åŠ è½½

```python
import pandas as pd
import glob
import re
from pathlib import Path
from datetime import datetime

class DataLoader:
    """
    æ™ºèƒ½æ•°æ®åŠ è½½å™¨ï¼Œè‡ªåŠ¨é€‚åº”æ•°æ®ç»“æ„å˜åŒ–
    """
    
    def __init__(self, base_path='data4dash'):
        self.base_path = Path(base_path)
        self.data_structure = self.discover_structure()
    
    def discover_structure(self):
        """è‡ªåŠ¨å‘ç°æ•°æ®ç»“æ„"""
        structure = {
            'available_years': [],
            'year_week_mapping': {},
            'file_paths': {},
            'last_scan': datetime.now().isoformat()
        }
        
        # æ‰«ææ‰€æœ‰å¹´åº¦æ–‡ä»¶å¤¹
        for year_dir in sorted(self.base_path.glob('data-*')):
            year_match = re.search(r'data-(\d{4})', year_dir.name)
            if not year_match:
                continue
                
            year = int(year_match.group(1))
            structure['available_years'].append(year)
            structure['year_week_mapping'][year] = []
            structure['file_paths'][year] = {}
            
            # æ‰«æå‘¨æ¬¡æ–‡ä»¶
            for csv_file in sorted(year_dir.glob('*.csv')):
                week_match = re.search(r'ç¬¬(\d+)å‘¨', csv_file.name)
                if week_match:
                    week = int(week_match.group(1))
                    structure['year_week_mapping'][year].append(week)
                    structure['file_paths'][year][week] = str(csv_file)
        
        return structure
    
    def load_single_file(self, year, week):
        """åŠ è½½å•ä¸ªæ–‡ä»¶"""
        if year not in self.data_structure['file_paths']:
            raise ValueError(f"å¹´åº¦ {year} çš„æ•°æ®ä¸å­˜åœ¨")
        
        if week not in self.data_structure['file_paths'][year]:
            available_weeks = self.data_structure['year_week_mapping'][year]
            raise ValueError(f"ç¬¬ {week} å‘¨æ•°æ®ä¸å­˜åœ¨ï¼Œå¯ç”¨å‘¨æ¬¡: {available_weeks}")
        
        file_path = self.data_structure['file_paths'][year][week]
        df = pd.read_csv(file_path)
        print(f"åŠ è½½ {year}å¹´ç¬¬{week}å‘¨æ•°æ®: {len(df)} è¡Œ, {len(df.columns)} åˆ—")
        return df
    
    def load_year_data(self, year, weeks=None):
        """åŠ è½½æŒ‡å®šå¹´åº¦æ•°æ®"""
        if year not in self.data_structure['available_years']:
            available_years = self.data_structure['available_years']
            raise ValueError(f"å¹´åº¦ {year} ä¸å­˜åœ¨ï¼Œå¯ç”¨å¹´åº¦: {available_years}")
        
        available_weeks = self.data_structure['year_week_mapping'][year]
        
        if weeks is None:
            weeks = available_weeks
        else:
            # éªŒè¯å‘¨æ¬¡æ˜¯å¦å­˜åœ¨
            invalid_weeks = set(weeks) - set(available_weeks)
            if invalid_weeks:
                print(f"è­¦å‘Š: å‘¨æ¬¡ {sorted(invalid_weeks)} ä¸å­˜åœ¨ï¼Œå°†è·³è¿‡")
            weeks = [w for w in weeks if w in available_weeks]
        
        dataframes = []
        for week in sorted(weeks):
            file_path = self.data_structure['file_paths'][year][week]
            df = pd.read_csv(file_path)
            dataframes.append(df)
        
        result = pd.concat(dataframes, ignore_index=True)
        print(f"åŠ è½½ {year}å¹´æ•°æ®: {len(weeks)} ä¸ªæ–‡ä»¶, æ€»è®¡ {len(result)} è¡Œ")
        return result
    
    def load_all_data(self, years=None):
        """åŠ è½½æ‰€æœ‰å¯ç”¨æ•°æ®"""
        if years is None:
            years = self.data_structure['available_years']
        
        dataframes = []
        for year in sorted(years):
            if year in self.data_structure['available_years']:
                year_data = self.load_year_data(year)
                dataframes.append(year_data)
            else:
                print(f"è­¦å‘Š: å¹´åº¦ {year} æ•°æ®ä¸å­˜åœ¨ï¼Œè·³è¿‡")
        
        if not dataframes:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ•°æ®æ–‡ä»¶")
        
        result = pd.concat(dataframes, ignore_index=True)
        print(f"åŠ è½½å®Œæˆ: {len(years)} ä¸ªå¹´åº¦, æ€»è®¡ {len(result)} è¡Œ")
        return result
    
    def get_data_summary(self):
        """è·å–æ•°æ®æ¦‚è§ˆ"""
        summary = {
            'total_years': len(self.data_structure['available_years']),
            'year_range': {
                'min': min(self.data_structure['available_years']) if self.data_structure['available_years'] else None,
                'max': max(self.data_structure['available_years']) if self.data_structure['available_years'] else None
            },
            'total_files': sum(len(weeks) for weeks in self.data_structure['year_week_mapping'].values()),
            'year_details': {}
        }
        
        for year, weeks in self.data_structure['year_week_mapping'].items():
            summary['year_details'][year] = {
                'week_count': len(weeks),
                'week_range': {'min': min(weeks), 'max': max(weeks)} if weeks else None,
                'missing_weeks': self._find_missing_weeks(weeks)
            }
        
        return summary
    
    def _find_missing_weeks(self, weeks):
        """æŸ¥æ‰¾ç¼ºå¤±çš„å‘¨æ¬¡"""
        if not weeks:
            return []
        
        min_week, max_week = min(weeks), max(weeks)
        expected = set(range(min_week, max_week + 1))
        actual = set(weeks)
        return sorted(list(expected - actual))

# ä½¿ç”¨ç¤ºä¾‹
loader = DataLoader('data4dash')

# è·å–æ•°æ®æ¦‚è§ˆ
summary = loader.get_data_summary()
print(f"æ•°æ®æ¦‚è§ˆ: {summary['total_years']} ä¸ªå¹´åº¦, {summary['total_files']} ä¸ªæ–‡ä»¶")

# åŠ è½½å•ä¸ªæ–‡ä»¶
df_single = loader.load_single_file(2024, 28)

# åŠ è½½æŒ‡å®šå¹´åº¦æ•°æ®
df_2024 = loader.load_year_data(2024)

# åŠ è½½æŒ‡å®šå‘¨æ¬¡èŒƒå›´
df_recent = loader.load_year_data(2024, weeks=[34, 35, 36])

# åŠ è½½æ‰€æœ‰æ•°æ®
df_all = loader.load_all_data()
```

#### 4.1.2 ä¼ ç»ŸåŠ è½½æ–¹å¼ï¼ˆå‘åå…¼å®¹ï¼‰

```python
# å•æ–‡ä»¶åŠ è½½
df = pd.read_csv('data4dash/data-2025/2025ä¿å•ç¬¬28å‘¨å˜åŠ¨æˆæœ¬æ˜ç»†è¡¨.csv')
print(f"æ•°æ®è§„æ¨¡: {len(df)} è¡Œ, {len(df.columns)} åˆ—")

# æ‰¹é‡åŠ è½½ï¼ˆéœ€è¦æ‰‹åŠ¨æŒ‡å®šè·¯å¾„ï¼‰
files_2025 = glob.glob('data4dash/data-2025/*.csv')
df_2025 = pd.concat([pd.read_csv(f) for f in files_2025], ignore_index=True)

# åŠ è½½æ‰€æœ‰å¹´åº¦æ•°æ®
all_files = glob.glob('data4dash/data-*/*.csv')
df_all = pd.concat([pd.read_csv(f) for f in all_files], ignore_index=True)

print(f"æ€»æ•°æ®é‡: {len(df_all)} è¡Œ")
```

#### 4.1.3 å†…å­˜ä¼˜åŒ–åŠ è½½
```python
# æŒ‡å®šæ•°æ®ç±»å‹ä»¥ä¼˜åŒ–å†…å­˜ä½¿ç”¨
dtype_dict = {
    'policy_start_year': 'int16',
    'week_number': 'int8',
    'policy_count': 'int32',
    'claim_case_count': 'int32',
    'is_new_energy_vehicle': 'bool',
    'is_transferred_vehicle': 'bool'
}

df = pd.read_csv('data4dash/data-2024/2024ä¿å•ç¬¬28å‘¨å˜åŠ¨æˆæœ¬æ˜ç»†è¡¨.csv', dtype=dtype_dict)
```

### 4.2 æ•°æ®è´¨é‡æ£€æŸ¥

```python
def data_quality_check(df):
    """æ•°æ®è´¨é‡æ£€æŸ¥å‡½æ•°"""
    report = {
        'total_records': len(df),
        'fields_count': len(df.columns),
        'completeness': {},
        'data_ranges': {}
    }
    
    # å®Œæ•´æ€§æ£€æŸ¥
    for col in df.columns:
        report['completeness'][col] = {
            'non_null_count': df[col].notna().sum(),
            'completeness_rate': df[col].notna().mean()
        }
    
    # å…³é”®å­—æ®µæ•°å€¼èŒƒå›´æ£€æŸ¥
    numeric_fields = [
        'signed_premium_yuan', 'matured_premium_yuan',
         'policy_count', 'claim_case_count', 'reported_claim_payment_yuan'
    ]
    
    for field in numeric_fields:
        if field in df.columns:
            report['data_ranges'][field] = {
                'min': df[field].min(),
                'max': df[field].max(),
                'mean': df[field].mean(),
                'zero_count': (df[field] == 0).sum()
            }
    
    return report
```

### 4.3 çµæ´»ç­›é€‰å‡½æ•°

```python
def filter_data(df, **kwargs):
    """
    çµæ´»çš„æ•°æ®ç­›é€‰å‡½æ•°
    
    å‚æ•°ç¤ºä¾‹:
        year=2025, branch='æˆéƒ½', insurance_type='å•†ä¸šä¿é™©'
        week_range=(28, 32), new_energy=True
    """
    result = df.copy()
    
    if 'year' in kwargs:
        result = result[result['policy_start_year'] == kwargs['year']]
    
    if 'branch' in kwargs:
        result = result[result['chengdu_branch'] == kwargs['branch']]
    
    if 'insurance_type' in kwargs:
        result = result[result['insurance_type'] == kwargs['insurance_type']]
    
    if 'week_range' in kwargs:
        start_week, end_week = kwargs['week_range']
        result = result[result['week_number'].between(start_week, end_week)]
    
    if 'new_energy' in kwargs:
        result = result[result['is_new_energy_vehicle'] == kwargs['new_energy']]
    
    return result

# ä½¿ç”¨ç¤ºä¾‹
filtered_data = filter_data(
    df, 
    year=2025, 
    branch='æˆéƒ½', 
    week_range=(28, 32),
    insurance_type='å•†ä¸šä¿é™©'
)
```

---

## 5. å…¸å‹åº”ç”¨åœºæ™¯

### 5.1 è¶‹åŠ¿åˆ†æ

#### 5.1.1 å‘¨åº¦ä¿è´¹è¶‹åŠ¿
```python
# æŒ‰å‘¨ç»Ÿè®¡ä¿è´¹è¶‹åŠ¿
weekly_trend = aggregate_and_calculate_metrics(df, ['week_number'])
weekly_trend = weekly_trend.sort_values('week_number')

# å¯è§†åŒ–è¶‹åŠ¿
import matplotlib.pyplot as plt
plt.figure(figsize=(12, 6))
plt.plot(weekly_trend['week_number'], weekly_trend['signed_premium_yuan'] / 10000)
plt.title('å‘¨åº¦ä¿è´¹è¶‹åŠ¿ï¼ˆä¸‡å…ƒï¼‰')
plt.xlabel('å‘¨æ•°')
plt.ylabel('ä¿è´¹ï¼ˆä¸‡å…ƒï¼‰')
plt.grid(True)
plt.show()
```

#### 5.1.2 èµ”ä»˜ç‡è¶‹åŠ¿åˆ†æ
```python
# æŒ‰å‘¨åˆ†æèµ”ä»˜ç‡å˜åŒ–
weekly_loss_ratio = aggregate_and_calculate_metrics(
    df, 
    ['week_number', 'insurance_type']
)

# åˆ†é™©ç§å±•ç¤ºè¶‹åŠ¿
for insurance_type in weekly_loss_ratio['insurance_type'].unique():
    data = weekly_loss_ratio[weekly_loss_ratio['insurance_type'] == insurance_type]
    plt.plot(data['week_number'], data['expired_loss_ratio_percent'], 
             label=insurance_type, marker='o')

plt.title('åˆ†é™©ç§èµ”ä»˜ç‡è¶‹åŠ¿')
plt.xlabel('å‘¨æ•°')
plt.ylabel('èµ”ä»˜ç‡ï¼ˆ%ï¼‰')
plt.legend()
plt.grid(True)
plt.show()
```

### 5.2 å¯¹æ¯”åˆ†æ

#### 5.2.1 æœºæ„ç»è¥å¯¹æ¯”
```python
# æŒ‰æœºæ„å¯¹æ¯”ç»è¥æŒ‡æ ‡
org_comparison = aggregate_and_calculate_metrics(
    df,
    ['chengdu_branch', 'insurance_type']
)

# è®¡ç®—æœºæ„æ’å
org_ranking = org_comparison.groupby('chengdu_branch').agg({
    'signed_premium_yuan': 'sum',
    'policy_count': 'sum',
    'reported_claim_payment_yuan': 'sum'
}).reset_index()

org_ranking = aggregate_and_calculate_metrics(org_ranking, [])
org_ranking = org_ranking.sort_values('signed_premium_yuan', ascending=False)

print("æœºæ„ç»è¥æ’å:")
print(org_ranking[[
    'chengdu_branch', 
    'signed_premium_yuan',
    'average_premium_per_policy_yuan', 
    'expired_loss_ratio_percent'
]])
```

#### 5.2.2 å®¢æˆ·ç±»å‹åˆ†æ
```python
# æŒ‰å®¢æˆ·ç±»å‹åˆ†æä¸šåŠ¡ç»“æ„
customer_analysis = aggregate_and_calculate_metrics(
    df,
    ['customer_category_3', 'insurance_type']
)

# è®¡ç®—å„å®¢æˆ·ç±»å‹çš„ä¸šåŠ¡è´¡çŒ®åº¦
customer_analysis['premium_contribution'] = (
    customer_analysis['signed_premium_yuan'] / 
    customer_analysis['signed_premium_yuan'].sum() * 100
)

# æŒ‰ä¿è´¹è´¡çŒ®åº¦æ’åº
customer_ranking = customer_analysis.sort_values(
    'premium_contribution', 
    ascending=False
)

print("å®¢æˆ·ç±»å‹ä¸šåŠ¡è´¡çŒ®åº¦:")
print(customer_ranking[[
    'customer_category_3',
    'insurance_type', 
    'signed_premium_yuan',
    'premium_contribution',
    'expired_loss_ratio_percent'
]].head(10))
```

### 5.3 é£é™©åˆ†æ

#### 5.3.1 é£é™©ç­‰çº§å‡ºé™©åˆ†æ
```python
# æŒ‰é£é™©ç­‰çº§åˆ†æå‡ºé™©è¡¨ç°
risk_analysis = df.groupby(['large_truck_score', 'insurance_type']).agg({
    'policy_count': 'sum',
    'claim_case_count': 'sum',
    'reported_claim_payment_yuan': 'sum',
    'signed_premium_yuan': 'sum'
}).reset_index()

risk_analysis = aggregate_and_calculate_metrics(risk_analysis, [])

# è¿‡æ»¤æœ‰æ•ˆæ•°æ®
risk_analysis = risk_analysis[
    (risk_analysis['large_truck_score'].notna()) & 
    (risk_analysis['policy_count'] > 100)  # æ ·æœ¬é‡è¿‡æ»¤
]

print("é£é™©ç­‰çº§å‡ºé™©åˆ†æ:")
print(risk_analysis[[
    'large_truck_score', 
    'insurance_type',
    'policy_count',
    'claim_frequency_percent', 
    'expired_loss_ratio_percent'
]].sort_values('claim_frequency_percent', ascending=False))
```

#### 5.3.2 æ–°èƒ½æºè½¦é£é™©å¯¹æ¯”
```python
# æ–°èƒ½æºè½¦ vs ä¼ ç»Ÿè½¦é£é™©å¯¹æ¯”
energy_comparison = aggregate_and_calculate_metrics(
    df,
    ['is_new_energy_vehicle', 'insurance_type']
)

# æ·»åŠ è½¦è¾†ç±»å‹æ ‡ç­¾
energy_comparison['vehicle_type'] = energy_comparison['is_new_energy_vehicle'].map({
    True: 'æ–°èƒ½æºè½¦',
    False: 'ä¼ ç»Ÿè½¦'
})

print("æ–°èƒ½æºè½¦é£é™©å¯¹æ¯”:")
print(energy_comparison[[
    'vehicle_type',
    'insurance_type',
    'policy_count',
    'average_premium_per_policy_yuan',
    'claim_frequency_percent',
    'expired_loss_ratio_percent'
]])
```

### 5.4 ç›ˆåˆ©èƒ½åŠ›åˆ†æ

```python
# ç»¼åˆç›ˆåˆ©èƒ½åŠ›åˆ†æ
profitability_analysis = aggregate_and_calculate_metrics(
    df,
    ['chengdu_branch', 'insurance_type', 'customer_category_3']
)

# è®¡ç®—è¾¹é™…è´¡çŒ®ï¼ˆç®€åŒ–ç‰ˆï¼Œå‡è®¾è´¹ç”¨ç‡ä¸º12%ï¼‰
profitability_analysis['estimated_expense_ratio'] = 12.0  # %
profitability_analysis['estimated_variable_cost_ratio'] = (
    profitability_analysis['expired_loss_ratio_percent'] + 
    profitability_analysis['estimated_expense_ratio']
)
profitability_analysis['estimated_margin_ratio'] = (
    100 - profitability_analysis['estimated_variable_cost_ratio']
)

# ç­›é€‰ç›ˆåˆ©èƒ½åŠ›è¾ƒå¥½çš„ä¸šåŠ¡
profitable_business = profitability_analysis[
    (profitability_analysis['estimated_margin_ratio'] > 20) &
    (profitability_analysis['policy_count'] > 50)
].sort_values('estimated_margin_ratio', ascending=False)

print("é«˜ç›ˆåˆ©ä¸šåŠ¡åˆ†æ:")
print(profitable_business[[
    'chengdu_branch',
    'insurance_type', 
    'customer_category_3',
    'policy_count',
    'signed_premium_yuan',
    'expired_loss_ratio_percent',
    'estimated_margin_ratio'
]].head(10))
```

---

## 6. æŠ€æœ¯æ³¨æ„äº‹é¡¹

### 6.1 æ•°æ®ç±»å‹å¤„ç†

```python
# æ—¥æœŸå­—æ®µè½¬æ¢
df['snapshot_date'] = pd.to_datetime(df['snapshot_date'])

# å¸ƒå°”å­—æ®µç¡®è®¤
print("å¸ƒå°”å­—æ®µå”¯ä¸€å€¼:")
print(f"is_new_energy_vehicle: {df['is_new_energy_vehicle'].unique()}")
print(f"is_transferred_vehicle: {df['is_transferred_vehicle'].unique()}")

# åˆ†ç±»å­—æ®µç¼–ç ï¼ˆå¦‚éœ€è¦ï¼‰
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['business_type_encoded'] = le.fit_transform(df['business_type_category'])
```

### 6.2 ç©ºå€¼å¤„ç†ç­–ç•¥

```python
# æ£€æŸ¥ç©ºå€¼æƒ…å†µ
null_summary = df.isnull().sum()
print("ç©ºå€¼ç»Ÿè®¡:")
print(null_summary[null_summary > 0])

# é£é™©è¯„çº§å­—æ®µç©ºå€¼å¤„ç†
risk_fields = [
    'vehicle_insurance_grade', 
    'highway_risk_grade', 
    'large_truck_score', 
    'small_truck_score'
]

for field in risk_fields:
    df[field] = df[field].fillna('æœªè¯„çº§')
    
# å•†ä¸šé™©å­—æ®µç©ºå€¼å¤„ç†ï¼ˆäº¤å¼ºé™©æ•°æ®ä¸­ä¸º0æ˜¯æ­£å¸¸çš„ï¼‰
df['commercial_premium_before_discount_yuan'] = df['commercial_premium_before_discount_yuan'].fillna(0)
```

### 6.3 æ€§èƒ½ä¼˜åŒ–å»ºè®® <mcreference link="https://www.askwisdom.ai/ai-data-preparation" index="3">3</mcreference>

```python
# 1. ä½¿ç”¨åˆ†å—å¤„ç†å¤§æ•°æ®
def process_large_dataset(file_path, chunk_size=10000):
    results = []
    for chunk in pd.read_csv(file_path, chunksize=chunk_size):
        # å¤„ç†æ¯ä¸ªæ•°æ®å—
        processed_chunk = aggregate_and_calculate_metrics(chunk, [])
        results.append(processed_chunk)
    
    return pd.concat(results, ignore_index=True)

# 2. ä½¿ç”¨å¹¶è¡Œå¤„ç†
from multiprocessing import Pool

def parallel_file_processing(file_list):
    with Pool() as pool:
        results = pool.map(pd.read_csv, file_list)
    return pd.concat(results, ignore_index=True)

# 3. ç¼“å­˜è®¡ç®—ç»“æœ
import pickle

def cache_calculation_result(df, cache_file):
    result = aggregate_and_calculate_metrics(df, [])
    with open(cache_file, 'wb') as f:
        pickle.dump(result, f)
    return result

def load_cached_result(cache_file):
    with open(cache_file, 'rb') as f:
        return pickle.load(f)
```

---

## 7. æ‰©å±•åŠŸèƒ½ä¸æœªæ¥å‘å±•

### 7.1 æ•°æ®æ›´æ–°æœºåˆ¶ä¸åŠ¨æ€é€‚åº”

#### 7.1.1 è‡ªåŠ¨åŒ–æ•°æ®ç®¡ç†

```python
import os
import json
import shutil
from datetime import datetime
from pathlib import Path

class DataManager:
    """
    æ•°æ®ç®¡ç†å™¨ï¼Œå¤„ç†åŠ¨æ€æ•°æ®æ›´æ–°å’Œç»´æŠ¤
    """
    
    def __init__(self, base_path='data4dash'):
        self.base_path = Path(base_path)
        self.metadata_path = self.base_path / 'metadata'
        self.ensure_structure()
    
    def ensure_structure(self):
        """ç¡®ä¿åŸºç¡€ç›®å½•ç»“æ„å­˜åœ¨"""
        self.base_path.mkdir(exist_ok=True)
        self.metadata_path.mkdir(exist_ok=True)
        
        # åˆ›å»ºå…ƒæ•°æ®æ–‡ä»¶
        for metadata_file in ['available_years.json', 'available_weeks.json', 'data_catalog.json']:
            file_path = self.metadata_path / metadata_file
            if not file_path.exists():
                file_path.write_text('{}', encoding='utf-8')
    
    def add_new_data_file(self, file_path, year=None, week=None):
        """æ·»åŠ æ–°çš„æ•°æ®æ–‡ä»¶"""
        source_path = Path(file_path)
        
        if not source_path.exists():
            raise FileNotFoundError(f"æºæ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        # è‡ªåŠ¨æå–å¹´åº¦å’Œå‘¨æ¬¡ä¿¡æ¯
        if year is None or week is None:
            filename = source_path.name
            year_match = re.search(r'(\d{4})', filename)
            week_match = re.search(r'ç¬¬(\d+)å‘¨', filename)
            
            if year_match:
                year = int(year_match.group(1))
            if week_match:
                week = int(week_match.group(1))
        
        if year is None or week is None:
            raise ValueError("æ— æ³•ä»æ–‡ä»¶åæå–å¹´åº¦æˆ–å‘¨æ¬¡ä¿¡æ¯")
        
        # åˆ›å»ºç›®æ ‡ç›®å½•
        target_dir = self.base_path / f'data-{year}'
        target_dir.mkdir(exist_ok=True)
        
        # ç”Ÿæˆæ ‡å‡†æ–‡ä»¶å
        target_filename = f'{year}ä¿å•ç¬¬{week:02d}å‘¨å˜åŠ¨æˆæœ¬æ˜ç»†è¡¨.csv'
        target_path = target_dir / target_filename
        
        # å¤åˆ¶æ–‡ä»¶
        shutil.copy2(source_path, target_path)
        
        # æ›´æ–°å…ƒæ•°æ®
        self.update_metadata()
        
        print(f"å·²æ·»åŠ æ•°æ®æ–‡ä»¶: {target_path}")
        return target_path
    
    def update_metadata(self):
        """æ›´æ–°å…ƒæ•°æ®ä¿¡æ¯"""
        loader = DataLoader(str(self.base_path))
        structure = loader.data_structure
        
        # æ›´æ–°å¯ç”¨å¹´åº¦
        years_file = self.metadata_path / 'available_years.json'
        years_data = {
            'years': structure['available_years'],
            'last_updated': datetime.now().isoformat()
        }
        years_file.write_text(json.dumps(years_data, indent=2, ensure_ascii=False), encoding='utf-8')
        
        # æ›´æ–°å‘¨æ¬¡æ˜ å°„
        weeks_file = self.metadata_path / 'available_weeks.json'
        weeks_data = {
            'year_week_mapping': structure['year_week_mapping'],
            'last_updated': datetime.now().isoformat()
        }
        weeks_file.write_text(json.dumps(weeks_data, indent=2, ensure_ascii=False), encoding='utf-8')
        
        # æ›´æ–°æ•°æ®ç›®å½•
        catalog_file = self.metadata_path / 'data_catalog.json'
        catalog_data = {
            'total_years': len(structure['available_years']),
            'total_files': sum(len(weeks) for weeks in structure['year_week_mapping'].values()),
            'file_paths': structure.get('file_paths', {}),
            'last_scan': datetime.now().isoformat()
        }
        catalog_file.write_text(json.dumps(catalog_data, indent=2, ensure_ascii=False), encoding='utf-8')
    
    def validate_data_integrity(self):
        """éªŒè¯æ•°æ®å®Œæ•´æ€§"""
        issues = []
        loader = DataLoader(str(self.base_path))
        
        for year, weeks in loader.data_structure['year_week_mapping'].items():
            if not weeks:
                issues.append(f"å¹´åº¦ {year} æ²¡æœ‰æ•°æ®æ–‡ä»¶")
                continue
            
            # æ£€æŸ¥ç¼ºå¤±å‘¨æ¬¡
            min_week, max_week = min(weeks), max(weeks)
            expected_weeks = set(range(min_week, max_week + 1))
            actual_weeks = set(weeks)
            missing_weeks = expected_weeks - actual_weeks
            
            if missing_weeks:
                issues.append(f"å¹´åº¦ {year} ç¼ºå¤±å‘¨æ¬¡: {sorted(missing_weeks)}")
            
            # æ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§
            for week in weeks:
                try:
                    df = loader.load_single_file(year, week)
                    if len(df) == 0:
                        issues.append(f"{year}å¹´ç¬¬{week}å‘¨æ•°æ®æ–‡ä»¶ä¸ºç©º")
                except Exception as e:
                    issues.append(f"{year}å¹´ç¬¬{week}å‘¨æ•°æ®æ–‡ä»¶æŸå: {str(e)}")
        
        return issues
    
    def archive_old_data(self, years_to_archive):
        """å½’æ¡£å†å²æ•°æ®"""
        archive_dir = self.base_path / 'archive'
        archive_dir.mkdir(exist_ok=True)
        
        for year in years_to_archive:
            source_dir = self.base_path / f'data-{year}'
            if source_dir.exists():
                target_dir = archive_dir / f'data-{year}'
                shutil.move(str(source_dir), str(target_dir))
                print(f"å·²å½’æ¡£ {year} å¹´åº¦æ•°æ®åˆ° {target_dir}")
        
        self.update_metadata()

# ä½¿ç”¨ç¤ºä¾‹
manager = DataManager('data4dash')

# æ·»åŠ æ–°æ•°æ®æ–‡ä»¶
manager.add_new_data_file('/path/to/new/data/2025ä¿å•ç¬¬37å‘¨å˜åŠ¨æˆæœ¬æ˜ç»†è¡¨.csv')

# éªŒè¯æ•°æ®å®Œæ•´æ€§
issues = manager.validate_data_integrity()
if issues:
    print("å‘ç°æ•°æ®é—®é¢˜:")
    for issue in issues:
        print(f"  - {issue}")
else:
    print("æ•°æ®å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡")

# å½’æ¡£æ—§æ•°æ®
# manager.archive_old_data([2022, 2023])
```

#### 7.1.2 æ ¸å¿ƒç‰¹æ€§

- **è‡ªåŠ¨å¹´åº¦è¯†åˆ«**: ç³»ç»Ÿè‡ªåŠ¨è¯†åˆ«æ–°å¹´åº¦æ•°æ®å¹¶åˆ›å»ºå¯¹åº”æ–‡ä»¶å¤¹
- **æ™ºèƒ½æ–‡ä»¶å‘½å**: è‡ªåŠ¨æ ‡å‡†åŒ–æ–‡ä»¶å‘½åæ ¼å¼ï¼Œç¡®ä¿ä¸€è‡´æ€§
- **å†å²æ•°æ®æ‰©å±•**: æ”¯æŒå‘å‰æ‰©å±•å†å²å¹´åº¦æ•°æ®ï¼ˆå¦‚2023å¹´åŠæ›´æ—©ï¼‰
- **æœªæ¥å…¼å®¹æ€§**: æ”¯æŒ2026å¹´åŠæœªæ¥å¹´åº¦æ•°æ®çš„è‡ªåŠ¨å¤„ç†
- **å¢é‡æ›´æ–°**: æ–°çš„å‘¨æ¬¡æ•°æ®å¯ç›´æ¥æ·»åŠ åˆ°å¯¹åº”å¹´åº¦æ–‡ä»¶å¤¹
- **105å‘¨è¦†ç›–**: ç³»ç»Ÿè®¾è®¡æ”¯æŒå®Œæ•´çš„105å‘¨æ•°æ®å‘¨æœŸç®¡ç†
- **æ•°æ®éªŒè¯**: è‡ªåŠ¨æ£€æŸ¥æ–°æ•°æ®çš„å­—æ®µå®Œæ•´æ€§å’Œæ ¼å¼ä¸€è‡´æ€§
- **å…ƒæ•°æ®ç®¡ç†**: å®æ—¶ç»´æŠ¤æ•°æ®ç›®å½•å’Œç´¢å¼•ä¿¡æ¯
- **å½’æ¡£æœºåˆ¶**: æ”¯æŒå†å²æ•°æ®çš„è‡ªåŠ¨å½’æ¡£å’Œç®¡ç†

#### 7.1.3 åŠ¨æ€é€‚åº”èƒ½åŠ›

- **ç»“æ„è‡ªå‘ç°**: æ— éœ€é¢„å®šä¹‰æ•°æ®ç»“æ„ï¼Œè‡ªåŠ¨é€‚åº”æ–°çš„å¹´åº¦å’Œå‘¨æ¬¡
- **ç¼ºå¤±æ•°æ®å¤„ç†**: æ™ºèƒ½è¯†åˆ«å’ŒæŠ¥å‘Šç¼ºå¤±çš„æ•°æ®æ–‡ä»¶
- **é”™è¯¯æ¢å¤**: æä¾›æ•°æ®éªŒè¯å’Œä¿®å¤æœºåˆ¶
- **æ‰©å±•æ€§è®¾è®¡**: æ”¯æŒæ–°ä¸šåŠ¡åœºæ™¯å’Œæ•°æ®æ ¼å¼çš„æ‰©å±•

### 7.2 AIæ¨¡å‹é›†æˆå»ºè®®

```python
# ç‰¹å¾å·¥ç¨‹ç¤ºä¾‹
def create_features_for_ml(df):
    """ä¸ºæœºå™¨å­¦ä¹ æ¨¡å‹åˆ›å»ºç‰¹å¾"""
    features = df.copy()
    
    # æ—¶é—´ç‰¹å¾
    features['quarter'] = (features['week_number'] - 1) // 13 + 1
    features['is_year_end'] = features['week_number'] > 48
    
    # ä¸šåŠ¡ç‰¹å¾
    features['is_commercial'] = features['insurance_type'] == 'å•†ä¸šä¿é™©'
    features['is_renewal'] = features['renewal_status'] == 'ç»­ä¿'
    
    # é£é™©ç‰¹å¾
    features['has_risk_grade'] = features['vehicle_insurance_grade'].notna()
    features['is_high_risk'] = features['large_truck_score'].isin(['D', 'E'])
    
    return features

# é¢„æµ‹æ¨¡å‹ç¤ºä¾‹æ¡†æ¶
from sklearn.ensemble import RandomForestRegressor

def build_loss_ratio_prediction_model(df):
    """æ„å»ºèµ”ä»˜ç‡é¢„æµ‹æ¨¡å‹"""
    features = create_features_for_ml(df)
    
    # é€‰æ‹©ç‰¹å¾åˆ—
    feature_columns = [
        'policy_start_year', 'week_number', 'quarter',
        'is_commercial', 'is_renewal', 'is_new_energy_vehicle',
        'has_risk_grade', 'is_high_risk'
    ]
    
    X = pd.get_dummies(features[feature_columns])
    y = features['expired_loss_ratio_percent']
    
    # è®­ç»ƒæ¨¡å‹
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X, y)
    
    return model, X.columns
```

### 7.3 å¯è§†åŒ–é›†æˆ

```python
# ä¸ä¸»æµå¯è§†åŒ–åº“é›†æˆ
import plotly.express as px
import plotly.graph_objects as go

def create_interactive_dashboard_data(df):
    """ä¸ºäº¤äº’å¼ä»ªè¡¨æ¿å‡†å¤‡æ•°æ®"""
    # æ—¶é—´åºåˆ—æ•°æ®
    time_series = aggregate_and_calculate_metrics(
        df, ['week_number', 'insurance_type']
    )
    
    # æœºæ„å¯¹æ¯”æ•°æ®
    org_comparison = aggregate_and_calculate_metrics(
        df, ['chengdu_branch', 'third_level_organization']
    )
    
    # é£é™©åˆ†å¸ƒæ•°æ®
    risk_distribution = aggregate_and_calculate_metrics(
        df, ['vehicle_insurance_grade', 'customer_category_3']
    )
    
    return {
        'time_series': time_series,
        'org_comparison': org_comparison,
        'risk_distribution': risk_distribution
    }

# Plotlyå›¾è¡¨ç¤ºä¾‹
def create_plotly_charts(dashboard_data):
    """åˆ›å»ºPlotlyäº¤äº’å›¾è¡¨"""
    # æ—¶é—´è¶‹åŠ¿å›¾
    fig_trend = px.line(
        dashboard_data['time_series'],
        x='week_number',
        y='signed_premium_yuan',
        color='insurance_type',
        title='ä¿è´¹è¶‹åŠ¿åˆ†æ'
    )
    
    # æœºæ„å¯¹æ¯”å›¾
    fig_org = px.bar(
        dashboard_data['org_comparison'],
        x='third_level_organization',
        y='expired_loss_ratio_percent',
        color='chengdu_branch',
        title='æœºæ„èµ”ä»˜ç‡å¯¹æ¯”'
    )
    
    return fig_trend, fig_org
```

---

## 8. æ€»ç»“

### 8.1 æ ¸å¿ƒä¼˜åŠ¿

1. **æ•°æ®ç»“æ„æ¸…æ™°**: 17ä¸ªç­›é€‰ç»´åº¦ + 9ä¸ªç»å¯¹å€¼å­—æ®µ + 7ä¸ªè®¡ç®—å­—æ®µçš„åˆ†å±‚è®¾è®¡
2. **è®¡ç®—å‡†ç¡®å¯é **: ä¸“ç”¨çš„ä»£ç è®¡ç®—å‡½æ•°ç¡®ä¿èšåˆåœºæ™¯ä¸‹çš„è®¡ç®—å‡†ç¡®æ€§
3. **æ‰©å±•æ€§å¼º**: æ”¯æŒæœªæ¥å¹´åº¦æ•°æ®è‡ªåŠ¨æ‰©å±•å’Œæ–°ä¸šåŠ¡åœºæ™¯é€‚é…
4. **AIå‹å¥½**: ç»“æ„åŒ–æ•°æ®æ ¼å¼ä¾¿äºAIç³»ç»Ÿè§£æå’Œå¤„ç† <mcreference link="https://docs.kapa.ai/improving/writing-best-practices" index="1">1</mcreference>

### 8.2 åº”ç”¨ä»·å€¼

- **ä¸šåŠ¡æ´å¯Ÿ**: æ”¯æŒå¤šç»´åº¦çš„è½¦é™©æˆæœ¬åˆ†æå’Œé£é™©è¯„ä¼°
- **å†³ç­–æ”¯æŒ**: ä¸ºç»è¥ç®¡ç†æä¾›æ•°æ®é©±åŠ¨çš„å†³ç­–ä¾æ®
- **æ•ˆç‡æå‡**: è‡ªåŠ¨åŒ–çš„è®¡ç®—å’Œåˆ†ææµç¨‹æé«˜å·¥ä½œæ•ˆç‡
- **é£é™©ç®¡æ§**: å®æ—¶çš„é£é™©æŒ‡æ ‡ç›‘æ§å’Œé¢„è­¦æœºåˆ¶

### 8.3 ä½¿ç”¨å»ºè®®

1. **å§‹ç»ˆä½¿ç”¨å®æ—¶è®¡ç®—**: é¿å…ç›´æ¥èšåˆç‡å€¼å­—æ®µ
2. **æ³¨æ„æ•°æ®è´¨é‡**: å®šæœŸæ£€æŸ¥æ•°æ®å®Œæ•´æ€§å’Œä¸€è‡´æ€§
3. **åˆç†é€‰æ‹©åˆ†æç»´åº¦**: æ ¹æ®ä¸šåŠ¡éœ€æ±‚é€‰æ‹©åˆé€‚çš„åˆ†ç»„ç»´åº¦
4. **å…³æ³¨æ€§èƒ½ä¼˜åŒ–**: å¤§æ•°æ®é‡æ—¶é‡‡ç”¨åˆ†å—å¤„ç†å’Œç¼“å­˜ç­–ç•¥

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2025-01-27  
**é€‚ç”¨ç³»ç»Ÿ**: è½¦é™©å˜åŠ¨æˆæœ¬å¤šç»´åˆ†æç³»ç»Ÿ  
**æŠ€æœ¯æ”¯æŒ**: Pythonä»£ç è®¡ç®— + Pandas + Python 3.8+  

---

*æœ¬æ–‡æ¡£æ•´åˆäº†æ•°æ®ç»“æ„è¯´æ˜ã€å­—æ®µåˆ†ç±»æ¸…å•ã€ä½¿ç”¨ç¤ºä¾‹å’ŒAIå¼€å‘æŒ‡å—ï¼Œä¸ºè½¦é™©å˜åŠ¨æˆæœ¬åˆ†æç³»ç»Ÿçš„å¼€å‘æä¾›å®Œæ•´çš„æ•°æ®åŸºç¡€å’ŒæŠ€æœ¯æŒ‡å¯¼ã€‚*